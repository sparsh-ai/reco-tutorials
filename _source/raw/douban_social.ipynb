{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PnaZzqWlO0c9"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/DeepGraphLearning/RecommenderSystems.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00q2pxOrRF49"
   },
   "outputs": [],
   "source": [
    "data_path = '/content/drive/My Drive/Recommendation/douban.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NILlbFfP9Uo"
   },
   "outputs": [],
   "source": [
    "# !tar -xzvf douban.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 727,
     "status": "ok",
     "timestamp": 1587297286472,
     "user": {
      "displayName": "Sparsh Agarwal",
      "photoUrl": "",
      "userId": "13037694610922482904"
     },
     "user_tz": -330
    },
    "id": "NK4CCJCVRu4_",
    "outputId": "8f589931-59c2-46a9-a767-60c98ad0f518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import argparse\n",
    "import random\n",
    "from collections import Counter\n",
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVlV9VXsRu3y"
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = './Douban/'\n",
    "\n",
    "SOCIAL_NETWORK_FILE = PATH_TO_DATA + 'socialnet/socialnet.tsv'\n",
    "RATING_FILE = PATH_TO_DATA + 'movie/douban_movie.tsv'\n",
    "\n",
    "max_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCLReDThSRVa"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsTIP_drSMp2"
   },
   "outputs": [],
   "source": [
    "def process_rating(day=7): # segment session in every $day days.\n",
    "    df = pd.read_csv(RATING_FILE, sep='\\t', dtype={0:str, 1:str, 2:np.int32, 3: np.float32})\n",
    "    df = df[df['Rating'].between(1,6,inclusive=True)]\n",
    "    span_left = 1.2e9\n",
    "    span_right = 1.485e9\n",
    "    df = df[df['Timestamp'].between(span_left, span_right, inclusive=True)]\n",
    "    min_timestamp = df['Timestamp'].min()\n",
    "    time_id = [int(math.floor((t-min_timestamp) / (86400*day))) for t in df['Timestamp']]\n",
    "    df['TimeId'] = time_id\n",
    "    session_id = [str(uid)+'_'+str(tid) for uid, tid in zip(df['UserId'], df['TimeId'])]\n",
    "    df['SessionId'] = session_id\n",
    "    print('Statistics of user ratings:')\n",
    "    print('\\tNumber of total ratings: {}'.format(len(df)))\n",
    "    print('\\tNumber of users: {}'.format(df.UserId.nunique()))\n",
    "    print('\\tNumber of items: {}'.format(df.ItemId.nunique()))\n",
    "    print('\\tAverage ratings per user:{}'.format(df.groupby('UserId').size().mean()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_4xcMk5lSVxf"
   },
   "outputs": [],
   "source": [
    "def process_social(): # read in social network.\n",
    "    net = pd.read_csv(SOCIAL_NETWORK_FILE, sep='\\t', dtype={0:str, 1: str})\n",
    "    net.drop_duplicates(subset=['Follower', 'Followee'], inplace=True)\n",
    "    friend_size = net.groupby('Follower').size()\n",
    "    #net = net[np.in1d(net.Follower, friend_size[friend_size>=5].index)]\n",
    "    print('Statistics of social network:')\n",
    "    print('\\tTotal user in social network:{}.\\n\\tTotal edges(links) in social network:{}.'.format(\\\n",
    "        net.Follower.nunique(), len(net)))\n",
    "    print('\\tAverage number of friends for users: {}'.format(net.groupby('Follower').size().mean()))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IqxVmX86SXbm"
   },
   "outputs": [],
   "source": [
    "def reset_id(data, id_map, column_name='UserId'):\n",
    "    mapped_id = data[column_name].map(id_map)\n",
    "    data[column_name] = mapped_id\n",
    "    if column_name == 'UserId':\n",
    "        session_id = [str(uid)+'_'+str(tid) for uid, tid in zip(data['UserId'], data['TimeId'])]\n",
    "        data['SessionId'] = session_id\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QO_gxyPpSY_C"
   },
   "outputs": [],
   "source": [
    "def split_data(day): #split data for training/validation/testing.\n",
    "    df_data = process_rating(day)\n",
    "    df_net = process_social()\n",
    "    df_net = df_net.loc[df_net['Follower'].isin(df_data['UserId'].unique())]\n",
    "    df_net = df_net.loc[df_net['Followee'].isin(df_data['UserId'].unique())]\n",
    "    df_data = df_data.loc[df_data['UserId'].isin(df_net.Follower.unique())]\n",
    "    \n",
    "    #restrict session length in [2, max_length]. We set a max_length because too long sequence may come from a fake user.\n",
    "    df_data = df_data[df_data['SessionId'].groupby(df_data['SessionId']).transform('size')>1]\n",
    "    df_data = df_data[df_data['SessionId'].groupby(df_data['SessionId']).transform('size')<=max_length]\n",
    "    #length_supports = df_data.groupby('SessionId').size()\n",
    "    #df_data = df_data[np.in1d(df_data.SessionId, length_supports[length_supports<=max_length].index)]\n",
    "    \n",
    "    # split train, test, valid.\n",
    "    tmax = df_data.TimeId.max()\n",
    "    session_max_times = df_data.groupby('SessionId').TimeId.max()\n",
    "    session_train = session_max_times[session_max_times < tmax - 26].index\n",
    "    session_holdout = session_max_times[session_max_times >= tmax - 26].index\n",
    "    train_tr = df_data[df_data['SessionId'].isin(session_train)] \n",
    "    holdout_data = df_data[df_data['SessionId'].isin(session_holdout)] \n",
    "    \n",
    "    print('Number of train/test: {}/{}'.format(len(train_tr), len(holdout_data)))\n",
    "   \n",
    "    train_tr = train_tr[train_tr['ItemId'].groupby(train_tr['ItemId']).transform('size')>=20]\n",
    "    train_tr = train_tr[train_tr['SessionId'].groupby(train_tr['SessionId']).transform('size')>1]\n",
    "    \n",
    "    print('Item size in train data: {}'.format(train_tr['ItemId'].nunique()))\n",
    "    train_item_counter = Counter(train_tr.ItemId)\n",
    "    to_predict = Counter(el for el in train_item_counter.elements() if train_item_counter[el] >= 50).keys()\n",
    "    print('Size of to predict: {}'.format(len(to_predict)))\n",
    "    \n",
    "    # split holdout to valid and test.\n",
    "    holdout_cn = holdout_data.SessionId.nunique()\n",
    "    holdout_ids = holdout_data.SessionId.unique()\n",
    "    np.random.shuffle(holdout_ids)\n",
    "    valid_cn = int(holdout_cn * 0.5)\n",
    "    session_valid = holdout_ids[0: valid_cn]\n",
    "    session_test = holdout_ids[valid_cn: ]\n",
    "    valid = holdout_data[holdout_data['SessionId'].isin(session_valid)]\n",
    "    test = holdout_data[holdout_data['SessionId'].isin(session_test)]\n",
    "\n",
    "    valid = valid[valid['ItemId'].isin(to_predict)]\n",
    "    valid = valid[valid['SessionId'].groupby(valid['SessionId']).transform('size')>1]\n",
    "    \n",
    "    test = test[test['ItemId'].isin(to_predict)]\n",
    "    test = test[test['SessionId'].groupby(test['SessionId']).transform('size')>1]\n",
    "\n",
    "    total_df = pd.concat([train_tr, valid, test])\n",
    "    df_net = df_net.loc[df_net['Follower'].isin(total_df['UserId'].unique())]\n",
    "    df_net = df_net.loc[df_net['Followee'].isin(total_df['UserId'].unique())]\n",
    "    user_map = dict(zip(total_df.UserId.unique(), range(total_df.UserId.nunique()))) \n",
    "    item_map = dict(zip(total_df.ItemId.unique(), range(1, 1+total_df.ItemId.nunique()))) \n",
    "    with open('user_id_map.tsv', 'w') as fout:\n",
    "        for k, v in user_map.items():\n",
    "            fout.write(str(k) + '\\t' + str(v) + '\\n')\n",
    "    with open('item_id_map.tsv', 'w') as fout:\n",
    "        for k, v in item_map.items():\n",
    "            fout.write(str(k) + '\\t' + str(v) + '\\n')\n",
    "    num_users = len(user_map)\n",
    "    num_items = len(item_map)\n",
    "    reset_id(total_df, user_map)\n",
    "    reset_id(train_tr, user_map)\n",
    "    reset_id(valid, user_map)\n",
    "    reset_id(test, user_map)\n",
    "    reset_id(df_net, user_map, 'Follower')\n",
    "    reset_id(df_net, user_map, 'Followee')\n",
    "    reset_id(total_df, item_map, 'ItemId')\n",
    "    reset_id(train_tr, item_map, 'ItemId')\n",
    "    reset_id(valid, item_map, 'ItemId')\n",
    "    reset_id(test, item_map, 'ItemId')\n",
    "    \n",
    "    print('Train set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}\\n\\tAvg length: {}'.format(len(train_tr), train_tr.SessionId.nunique(), train_tr.ItemId.nunique(), train_tr.groupby('SessionId').size().mean()))\n",
    "    print('Valid set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}\\n\\tAvg length: {}'.format(len(valid), valid.SessionId.nunique(), valid.ItemId.nunique(), valid.groupby('SessionId').size().mean()))\n",
    "    print('Test set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}\\n\\tAvg length: {}'.format(len(test), test.SessionId.nunique(), test.ItemId.nunique(), test.groupby('SessionId').size().mean()))\n",
    "    user2sessions = total_df.groupby('UserId')['SessionId'].apply(set).to_dict()\n",
    "    user_latest_session = []\n",
    "    for idx in range(num_users):\n",
    "        sessions = user2sessions[idx]\n",
    "        latest = []\n",
    "        for t in range(tmax+1):\n",
    "            if t == 0:\n",
    "                latest.append('NULL')\n",
    "            else:\n",
    "                sess_id_tmp = str(idx) + '_' + str(t-1)\n",
    "                if sess_id_tmp in sessions:\n",
    "                    latest.append(sess_id_tmp)\n",
    "                else:\n",
    "                    latest.append(latest[t-1])\n",
    "        user_latest_session.append(latest)\n",
    "    \n",
    "    train_tr.to_csv('train.tsv', sep='\\t', index=False)\n",
    "    valid.to_csv('valid.tsv', sep='\\t', index=False)\n",
    "    test.to_csv('test.tsv', sep='\\t', index=False)\n",
    "    df_net.to_csv('adj.tsv', sep='\\t', index=False)\n",
    "    with open('latest_sessions.txt', 'w') as fout:\n",
    "        for idx in range(num_users):\n",
    "            fout.write(','.join(user_latest_session[idx]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66719,
     "status": "ok",
     "timestamp": 1587297619220,
     "user": {
      "displayName": "Sparsh Agarwal",
      "photoUrl": "",
      "userId": "13037694610922482904"
     },
     "user_tz": -330
    },
    "id": "OrZQHFA0Si-Z",
    "outputId": "6f11a4c2-46d6-4667-dc10-09430ef5fca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of user ratings:\n",
      "\tNumber of total ratings: 8483267\n",
      "\tNumber of users: 82901\n",
      "\tNumber of items: 73677\n",
      "\tAverage ratings per user:102.33009251999373\n",
      "Statistics of social network:\n",
      "\tTotal user in social network:112679.\n",
      "\tTotal edges(links) in social network:1758302.\n",
      "\tAverage number of friends for users: 15.604522581847549\n",
      "Number of train/test: 2916327/134476\n",
      "Item size in train data: 12591\n",
      "Size of to predict: 7531\n",
      "Train set\n",
      "\tEvents: 2717619\n",
      "\tSessions: 650675\n",
      "\tItems: 12591\n",
      "\tAvg length: 4.176615053598186\n",
      "Valid set\n",
      "\tEvents: 29932\n",
      "\tSessions: 8033\n",
      "\tItems: 4901\n",
      "\tAvg length: 3.7261297149259307\n",
      "Test set\n",
      "\tEvents: 29519\n",
      "\tSessions: 8009\n",
      "\tItems: 4822\n",
      "\tAvg length: 3.685728555375203\n"
     ]
    }
   ],
   "source": [
    "day = 7\n",
    "split_data(day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlUsdJF_UB7n"
   },
   "source": [
    "UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G64IZdB2SxJb"
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def load_adj(data_path):\n",
    "    df_adj = pd.read_csv(data_path + '/adj.tsv', sep='\\t', dtype={0:np.int32, 1:np.int32})\n",
    "    return df_adj    \n",
    "\n",
    "def load_latest_session(data_path):\n",
    "    ret = []\n",
    "    for line in open(data_path + '/latest_sessions.txt'):\n",
    "        chunks = line.strip().split(',')\n",
    "        ret.append(chunks)\n",
    "    return ret\n",
    "\n",
    "def load_map(data_path, name='user'):\n",
    "    if name == 'user':\n",
    "        file_path = data_path + '/user_id_map.tsv'\n",
    "    elif name == 'item':\n",
    "        file_path = data_path + '/item_id_map.tsv'\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    id_map = {}\n",
    "    for line in open(file_path):\n",
    "        k, v = line.strip().split('\\t')\n",
    "        id_map[k] = str(v)\n",
    "    return id_map\n",
    "\n",
    "def load_data(data_path):\n",
    "    adj = load_adj(data_path)\n",
    "    latest_sessions = load_latest_session(data_path)\n",
    "    user_id_map = load_map(data_path, 'user')\n",
    "    item_id_map = load_map(data_path, 'item')\n",
    "    train = pd.read_csv(data_path + '/train.tsv', sep='\\t', dtype={0:np.int32, 1:np.int32, 3:np.float32})\n",
    "    valid = pd.read_csv(data_path + '/valid.tsv', sep='\\t', dtype={0:np.int32, 1:np.int32, 3:np.float32})\n",
    "    test = pd.read_csv(data_path + '/test.tsv', sep='\\t', dtype={0:np.int32, 1:np.int32, 3:np.float32})\n",
    "    return [adj, latest_sessions, user_id_map, item_id_map, train, valid, test]\n",
    "\n",
    "data_path = '/content'\n",
    "data = load_data(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6fLxW5S9Ui6h"
   },
   "source": [
    "INITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwJra7jCUaQt"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# DISCLAIMER:\n",
    "# This file is derived from \n",
    "# https://github.com/tkipf/gcn\n",
    "# which is also under the MIT license\n",
    "\n",
    "def uniform(shape, scale=0.05, name=None):\n",
    "    \"\"\"Uniform init.\"\"\"\n",
    "    initial = tf.random_uniform(shape, minval=-scale, maxval=scale, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def glorot(shape, name=None):\n",
    "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def ones(shape, name=None):\n",
    "    \"\"\"All ones.\"\"\"\n",
    "    initial = tf.ones(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQGzN9GlVKlS"
   },
   "source": [
    "Neigh Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8HTbjCKVOVP"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classes that are used to sample node neighborhoods\n",
    "\"\"\"\n",
    "\n",
    "class UniformNeighborSampler(object):\n",
    "    \"\"\"\n",
    "    Uniformly samples neighbors.\n",
    "    Assumes that adj lists are padded with random re-sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, adj_info, visible_time, deg):\n",
    "        self.adj_info = adj_info\n",
    "        self.visible_time = visible_time\n",
    "        self.deg = deg\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        nodeids, num_samples, timeids, first_or_second, support_size = inputs\n",
    "        adj_lists = []\n",
    "        for idx in range(len(nodeids)):\n",
    "            node = nodeids[idx]\n",
    "            timeid = timeids[idx // support_size]\n",
    "            adj = self.adj_info[node, :]\n",
    "            neighbors = []\n",
    "            for neighbor in adj:\n",
    "                if first_or_second == 'second':\n",
    "                    if self.visible_time[neighbor] <= timeid:\n",
    "                        neighbors.append(neighbor)\n",
    "                elif first_or_second == 'first':\n",
    "                    if self.visible_time[neighbor] <= timeid and self.deg[neighbor] > 0:\n",
    "                        for second_neighbor in self.adj_info[neighbor]:\n",
    "                            if self.visible_time[second_neighbor] <= timeid:\n",
    "                                neighbors.append(neighbor)\n",
    "                                break\n",
    "            assert len(neighbors) > 0\n",
    "            if len(neighbors) < num_samples:\n",
    "                neighbors = np.random.choice(neighbors, num_samples, replace=True)\n",
    "            elif len(neighbors) > num_samples:\n",
    "                neighbors = np.random.choice(neighbors, num_samples, replace=False)\n",
    "            adj_lists.append(neighbors)\n",
    "        return np.array(adj_lists, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ktvepo3ZU_Sa"
   },
   "source": [
    "LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0JCgKyjUkxC"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# DISCLAIMER:\n",
    "# This file is forked from \n",
    "# https://github.com/tkipf/gcn\n",
    "# which is also under the MIT license\n",
    "\n",
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging', 'model_size'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, dropout=0., weight_decay=0.,\n",
    "                 act=tf.nn.relu, placeholders=None, bias=True, featureless=False, \n",
    "                 sparse_inputs=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.act = act\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        if sparse_inputs:\n",
    "            self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = tf.get_variable('weights', shape=(input_dim, output_dim),\n",
    "                                         dtype=tf.float32, \n",
    "                                         initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                         regularizer=tf.contrib.layers.l2_regularizer(self.weight_decay))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # transform\n",
    "        output = tf.matmul(x, self.vars['weights'])\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZQXjJjuVWMy"
   },
   "source": [
    "MINI BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 130992,
     "status": "ok",
     "timestamp": 1587298255874,
     "user": {
      "displayName": "Sparsh Agarwal",
      "photoUrl": "",
      "userId": "13037694610922482904"
     },
     "user_tz": -330
    },
    "id": "Ini7aWYCU8QI",
    "outputId": "4d047e54-100f-4140-9dd3-3863453a37b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sessions: 613221\tratings: 2554567\n",
      "sessions: 7354\tratings: 27185\n",
      "sessions: 7304\tratings: 26705\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "class MinibatchIterator(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                adj_info, # in pandas dataframe\n",
    "                latest_sessions,\n",
    "                data, # data list, either [train, valid] or [train, valid, test].\n",
    "                placeholders,\n",
    "                batch_size,\n",
    "                max_degree,\n",
    "                num_nodes,\n",
    "                max_length=20,\n",
    "                samples_1_2=[10,5],\n",
    "                training=True):\n",
    "        self.num_layers = 2 # Currently, only 2 layer is supported.\n",
    "        self.adj_info = adj_info\n",
    "        self.latest_sessions = latest_sessions\n",
    "        self.training = training\n",
    "        self.train_df, self.valid_df, self.test_df = data\n",
    "        self.all_data = pd.concat(data)\n",
    "        self.placeholders = placeholders\n",
    "        self.batch_size = batch_size\n",
    "        self.max_degree = max_degree\n",
    "        self.num_nodes = num_nodes\n",
    "        self.max_length = max_length\n",
    "        self.samples_1_2 = samples_1_2\n",
    "        self.sizes = [1, samples_1_2[1], samples_1_2[1]*samples_1_2[0]]\n",
    "        self.visible_time = self.user_visible_time()\n",
    "        self.test_adj, self.test_deg = self.construct_test_adj()\n",
    "        if self.training:\n",
    "            self.adj, self.deg = self.construct_adj()\n",
    "            self.train_session_ids = self._remove_infoless(self.train_df, self.adj, self.deg)\n",
    "            self.valid_session_ids = self._remove_infoless(self.valid_df, self.test_adj, self.test_deg)\n",
    "            self.sampler = UniformNeighborSampler(self.adj, self.visible_time, self.deg)\n",
    "        \n",
    "        self.test_session_ids = self._remove_infoless(self.test_df, self.test_adj, self.test_deg)\n",
    "       \n",
    "        self.padded_data, self.mask = self._padding_sessions(self.all_data)\n",
    "        self.test_sampler = UniformNeighborSampler(self.test_adj, self.visible_time, self.test_deg)\n",
    "        \n",
    "        self.batch_num = 0\n",
    "        self.batch_num_val = 0\n",
    "        self.batch_num_test = 0\n",
    "\n",
    "    def user_visible_time(self):\n",
    "        '''\n",
    "            Find out when each user is 'visible' to her friends, i.e., every user's first click/watching time.\n",
    "        '''\n",
    "        visible_time = []\n",
    "        for l in self.latest_sessions:\n",
    "            timeid = max(loc for loc, val in enumerate(l) if val == 'NULL') + 1\n",
    "            visible_time.append(timeid)\n",
    "            assert timeid > 0 and timeid < len(l), 'Wrong when create visible time {}'.format(timeid)\n",
    "        return visible_time\n",
    "\n",
    "    def _remove_infoless(self, data, adj, deg):\n",
    "        '''\n",
    "        Remove users who have no sufficient friends.\n",
    "        '''\n",
    "        data = data.loc[deg[data['UserId']] != 0]\n",
    "        reserved_session_ids = []\n",
    "        print('sessions: {}\\tratings: {}'.format(data.SessionId.nunique(), len(data)))\n",
    "        for sessid in data.SessionId.unique():\n",
    "            userid, timeid = sessid.split('_')\n",
    "            userid, timeid = int(userid), int(timeid)\n",
    "            cn_1 = 0\n",
    "            for neighbor in adj[userid, : ]:\n",
    "                if self.visible_time[neighbor] <= timeid and deg[neighbor] > 0:\n",
    "                    cn_2 = 0\n",
    "                    for second_neighbor in adj[neighbor, : ]:\n",
    "                        if self.visible_time[second_neighbor] <= timeid:\n",
    "                            break\n",
    "                        cn_2 += 1\n",
    "                    if cn_2 < self.max_degree:\n",
    "                        break\n",
    "                cn_1 += 1\n",
    "            if cn_1 < self.max_degree:\n",
    "                reserved_session_ids.append(sessid)\n",
    "        return reserved_session_ids\n",
    "\n",
    "    def _padding_sessions(self, data):\n",
    "        '''\n",
    "        Pad zeros at the end of each session to length self.max_length for batch training.\n",
    "        '''\n",
    "        data = data.sort_values(by=['TimeId']).groupby('SessionId')['ItemId'].apply(list).to_dict()\n",
    "        new_data = {}\n",
    "        data_mask = {}\n",
    "        for k, v in data.items():\n",
    "            mask = np.ones(self.max_length, dtype=np.float32)\n",
    "            x = v[:-1]\n",
    "            y = v[1: ]\n",
    "            assert len(x) > 0\n",
    "            padded_len = self.max_length - len(x)\n",
    "            if padded_len > 0:\n",
    "                x.extend([0] * padded_len)\n",
    "                y.extend([0] * padded_len)\n",
    "                mask[-padded_len: ] = 0.\n",
    "            v.extend([0] * (self.max_length - len(v)))\n",
    "            x = x[:self.max_length]\n",
    "            y = y[:self.max_length]\n",
    "            v = v[:self.max_length]\n",
    "            new_data[k] = [np.array(x, dtype=np.int32), np.array(y, dtype=np.int32), np.array(v, dtype=np.int32)]\n",
    "            data_mask[k] = np.array(mask, dtype=bool)\n",
    "        return new_data, data_mask\n",
    "\n",
    "    def _batch_feed_dict(self, current_batch):\n",
    "        '''\n",
    "        Construct batch inputs.\n",
    "        '''\n",
    "        current_batch_sess_ids, samples, support_sizes = current_batch\n",
    "        feed_dict = {}\n",
    "        input_x = []\n",
    "        input_y = []\n",
    "        mask_y = []\n",
    "        timeids = []\n",
    "        for sessid in current_batch_sess_ids:\n",
    "            nodeid, timeid = sessid.split('_')\n",
    "            timeids.append(int(timeid))\n",
    "            x, y, _ = self.padded_data[sessid]\n",
    "            mask = self.mask[sessid]\n",
    "            input_x.append(x)\n",
    "            input_y.append(y)\n",
    "            mask_y.append(mask)\n",
    "        feed_dict.update({self.placeholders['input_x']: input_x})\n",
    "        feed_dict.update({self.placeholders['input_y']: input_y})\n",
    "        feed_dict.update({self.placeholders['mask_y']: mask_y})\n",
    "\n",
    "        feed_dict.update({self.placeholders['support_nodes_layer1']: samples[2]})\n",
    "        feed_dict.update({self.placeholders['support_nodes_layer2']: samples[1]})\n",
    "        #prepare sopportive user's recent sessions.\n",
    "        support_layers_session = []\n",
    "        support_layers_length = []\n",
    "        for layer in range(self.num_layers):\n",
    "            start = 0\n",
    "            t = self.num_layers - layer\n",
    "            support_sessions = []\n",
    "            support_lengths = []\n",
    "            for batch in range(self.batch_size):\n",
    "                timeid = timeids[batch]\n",
    "                support_nodes = samples[t][start: start + support_sizes[t]]\n",
    "                for support_node in support_nodes:\n",
    "                    support_session_id = str(self.latest_sessions[support_node][timeid])\n",
    "                    support_session = self.padded_data[support_session_id][2]\n",
    "                    #print(support_session)\n",
    "                    length = np.count_nonzero(support_session)\n",
    "                    support_sessions.append(support_session)\n",
    "                    support_lengths.append(length)\n",
    "                start += support_sizes[t]\n",
    "            support_layers_session.append(support_sessions)\n",
    "            support_layers_length.append(support_lengths)\n",
    "        feed_dict.update({self.placeholders['support_sessions_layer1']:support_layers_session[0]})\n",
    "        feed_dict.update({self.placeholders['support_sessions_layer2']:support_layers_session[1]})\n",
    "        feed_dict.update({self.placeholders['support_lengths_layer1']:support_layers_length[0]})\n",
    "        feed_dict.update({self.placeholders['support_lengths_layer2']:support_layers_length[1]})\n",
    "        return feed_dict \n",
    "\n",
    "    def sample(self, nodeids, timeids, sampler):\n",
    "        '''\n",
    "        Sample neighbors recursively. First-order, then second-order, ...\n",
    "        '''\n",
    "        samples = [nodeids]\n",
    "        support_size = 1\n",
    "        support_sizes = [support_size]\n",
    "        first_or_second = ['second', 'first']\n",
    "        for k in range(self.num_layers):\n",
    "            t = self.num_layers - k - 1\n",
    "            node = sampler([samples[k], self.samples_1_2[t], timeids, first_or_second[t], support_size])\n",
    "            support_size *= self.samples_1_2[t]\n",
    "            samples.append(np.reshape(node, [support_size * self.batch_size,]))\n",
    "            support_sizes.append(support_size)\n",
    "        return samples, support_sizes\n",
    "\n",
    "    def next_val_minibatch_feed_dict(self, val_or_test='val'):\n",
    "        '''\n",
    "        Construct evaluation or test inputs.\n",
    "        '''\n",
    "        if val_or_test == 'val':\n",
    "            start = self.batch_num_val * self.batch_size\n",
    "            self.batch_num_val += 1\n",
    "            data = self.valid_session_ids\n",
    "        elif val_or_test == 'test':\n",
    "            start = self.batch_num_test * self.batch_size\n",
    "            self.batch_num_test += 1\n",
    "            data = self.test_session_ids\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        current_batch_sessions = data[start: start + self.batch_size]\n",
    "        nodes = [int(sessionid.split('_')[0]) for sessionid in current_batch_sessions]\n",
    "        timeids = [int(sessionid.split('_')[1]) for sessionid in current_batch_sessions]\n",
    "        samples, support_sizes = self.sample(nodes, timeids, self.test_sampler)\n",
    "        return self._batch_feed_dict([current_batch_sessions, samples, support_sizes])\n",
    "\n",
    "    def next_train_minibatch_feed_dict(self):\n",
    "        '''\n",
    "        Generate next training batch data.\n",
    "        '''\n",
    "        start = self.batch_num * self.batch_size\n",
    "        self.batch_num += 1\n",
    "        current_batch_sessions = self.train_session_ids[start: start + self.batch_size]\n",
    "        nodes = [int(sessionid.split('_')[0]) for sessionid in current_batch_sessions]\n",
    "        timeids = [int(sessionid.split('_')[1]) for sessionid in current_batch_sessions]\n",
    "        samples, support_sizes = self.sample(nodes, timeids, self.sampler)\n",
    "        return self._batch_feed_dict([current_batch_sessions, samples, support_sizes])\n",
    "\n",
    "    def construct_adj(self):\n",
    "        '''\n",
    "        Construct adj table used during training.\n",
    "        '''\n",
    "        adj = self.num_nodes*np.ones((self.num_nodes+1, self.max_degree), dtype=np.int32)\n",
    "        deg = np.zeros((self.num_nodes,))\n",
    "        missed = 0\n",
    "        for nodeid in self.train_df.UserId.unique():\n",
    "            neighbors = np.array([neighbor for neighbor in \n",
    "                                self.adj_info.loc[self.adj_info['Follower']==nodeid].Followee.unique()], dtype=np.int32)\n",
    "            deg[nodeid] = len(neighbors)\n",
    "            if len(neighbors) == 0:\n",
    "                missed += 1\n",
    "                continue\n",
    "            if len(neighbors) > self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "            elif len(neighbors) < self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            adj[nodeid, :] = neighbors\n",
    "        #print('Unexpected missing during constructing adj list: {}'.format(missed))\n",
    "        return adj, deg\n",
    "\n",
    "    def construct_test_adj(self):\n",
    "        '''\n",
    "        Construct adj table used during evaluation or testing.\n",
    "        '''\n",
    "        adj = self.num_nodes*np.ones((self.num_nodes+1, self.max_degree), dtype=np.int32)\n",
    "        deg = np.zeros((self.num_nodes,))\n",
    "        missed = 0\n",
    "        data = self.all_data\n",
    "        for nodeid in data.UserId.unique():\n",
    "            neighbors = np.array([neighbor for neighbor in \n",
    "                                self.adj_info.loc[self.adj_info['Follower']==nodeid].Followee.unique()], dtype=np.int32)\n",
    "            deg[nodeid] = len(neighbors)\n",
    "            if len(neighbors) == 0:\n",
    "                missed += 1\n",
    "                continue\n",
    "            if len(neighbors) > self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "            elif len(neighbors) < self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            adj[nodeid, :] = neighbors\n",
    "        #print('Unexpected missing during constructing adj list: {}'.format(missed))\n",
    "        return adj, deg\n",
    "\n",
    "    def end(self):\n",
    "        '''\n",
    "        Indicate whether we finish a pass over all training samples.\n",
    "        '''\n",
    "        return self.batch_num * self.batch_size > len(self.train_session_ids) - self.batch_size\n",
    "    \n",
    "    def end_val(self, val_or_test='val'):\n",
    "        '''\n",
    "        Indicate whether we finish a pass over all testing or evaluation samples.\n",
    "        '''\n",
    "        batch_num = self.batch_num_val if val_or_test == 'val' else self.batch_num_test\n",
    "        data = self.valid_session_ids if val_or_test == 'val' else self.test_session_ids\n",
    "        end = batch_num * self.batch_size > len(data) - self.batch_size\n",
    "        if end:\n",
    "            if val_or_test == 'val':\n",
    "                self.batch_num_val = 0\n",
    "            elif val_or_test == 'test':\n",
    "                self.batch_num_test = 0\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        if end:\n",
    "            self.batch_num_val = 0\n",
    "        return end\n",
    "\n",
    "    def shuffle(self):\n",
    "        '''\n",
    "        Shuffle training data.\n",
    "        '''\n",
    "        self.train_session_ids = np.random.permutation(self.train_session_ids)\n",
    "        self.batch_num = 0\n",
    "\n",
    "data = load_data('/content')\n",
    "adj_info = data[0]\n",
    "latest_per_user_by_time = data[1]\n",
    "user_id_map = data[2]\n",
    "item_id_map = data[3]\n",
    "train_df = data[4]\n",
    "valid_df = data[5]\n",
    "test_df = data[6]\n",
    "\n",
    "minibatch = MinibatchIterator(adj_info,\n",
    "            latest_per_user_by_time,\n",
    "            [train_df, valid_df, test_df],\n",
    "            None, #placeholders,\n",
    "            batch_size=1,\n",
    "            max_degree=50,\n",
    "            num_nodes=len(user_id_map),\n",
    "            max_length=20,\n",
    "            samples_1_2=[10, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8mylU4fWsbX"
   },
   "source": [
    "AGGREGATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpGBiAF_V0PS"
   },
   "outputs": [],
   "source": [
    "# Mean, MaxPool, GCN aggregators are collected from \n",
    "# https://github.com/williamleif/GraphSAGE\n",
    "# which is also under the MIT license\n",
    "\n",
    "class MeanAggregator(Layer):\n",
    "    \"\"\"\n",
    "    Aggregates via mean followed by matmul and non-linearity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, \n",
    "            name=None, concat=False, **kwargs):\n",
    "        super(MeanAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['neigh_weights'] = glorot([neigh_input_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "            self.vars['self_weights'] = glorot([input_dim, output_dim],\n",
    "                                                        name='self_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "\n",
    "        neigh_vecs = tf.nn.dropout(neigh_vecs, 1-self.dropout)\n",
    "        self_vecs = tf.nn.dropout(self_vecs, 1-self.dropout)\n",
    "        neigh_means = tf.reduce_mean(neigh_vecs, axis=1)\n",
    "       \n",
    "        # [nodes] x [out_dim]\n",
    "        from_neighs = tf.matmul(neigh_means, self.vars['neigh_weights'])\n",
    "\n",
    "        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n",
    "         \n",
    "        if not self.concat:\n",
    "            output = tf.add_n([from_self, from_neighs])\n",
    "        else:\n",
    "            output = tf.concat([from_self, from_neighs], axis=1)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "       \n",
    "        return self.act(output)\n",
    "\n",
    "class GCNAggregator(Layer):\n",
    "    \"\"\"\n",
    "    Aggregates via mean followed by matmul and non-linearity.\n",
    "    Same matmul parameters are used self vector and neighbor vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, name=None, concat=False, **kwargs):\n",
    "        super(GCNAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['weights'] = glorot([neigh_input_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "\n",
    "        neigh_vecs = tf.nn.dropout(neigh_vecs, 1-self.dropout)\n",
    "        self_vecs = tf.nn.dropout(self_vecs, 1-self.dropout)\n",
    "        means = tf.reduce_mean(tf.concat([neigh_vecs, \n",
    "            tf.expand_dims(self_vecs, axis=1)], axis=1), axis=1)\n",
    "       \n",
    "        # [nodes] x [out_dim]\n",
    "        output = tf.matmul(means, self.vars['weights'])\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "       \n",
    "        return self.act(output)\n",
    "\n",
    "\n",
    "class AttentionAggregator(Layer):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, name=None, concat=False, **kwargs):\n",
    "        super(AttentionAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['weights'] = glorot([neigh_input_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='neigh_bias')\n",
    "\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "\n",
    "        neigh_vecs = tf.nn.dropout(neigh_vecs, 1-self.dropout)\n",
    "        self_vecs = tf.nn.dropout(self_vecs, 1-self.dropout)\n",
    "        \n",
    "        # Reshape from [batch_size, depth] to [batch_size, 1, depth] for matmul.\n",
    "        query = tf.expand_dims(self_vecs, 1)\n",
    "        neigh_self_vecs = tf.concat([neigh_vecs, query], axis=1)\n",
    "        score = tf.matmul(query, neigh_self_vecs, transpose_b=True)\n",
    "        score = tf.nn.softmax(score, dim=-1)\n",
    "\n",
    "        # alignment(score) shape is [batch_size, 1, depth]\n",
    "        context = tf.matmul(score, neigh_self_vecs)\n",
    "        context = tf.squeeze(context, [1])\n",
    "\n",
    "        # [nodes] x [out_dim]\n",
    "        output = tf.matmul(context, self.vars['weights'])\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "       \n",
    "        return self.act(output)\n",
    "\n",
    "\n",
    "class MaxPoolingAggregator(Layer):\n",
    "    \"\"\" Aggregates via max-pooling over MLP functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, model_size=\"small\", neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, name=None, concat=False, **kwargs):\n",
    "        super(MaxPoolingAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        if model_size == \"small\":\n",
    "            hidden_dim = self.hidden_dim = 512\n",
    "        elif model_size == \"big\":\n",
    "            hidden_dim = self.hidden_dim = 1024\n",
    "\n",
    "        self.mlp_layers = []\n",
    "        self.mlp_layers.append(Dense(input_dim=neigh_input_dim,\n",
    "                                 output_dim=hidden_dim,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=dropout,\n",
    "                                 sparse_inputs=False,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['neigh_weights'] = glorot([hidden_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "           \n",
    "            self.vars['self_weights'] = glorot([input_dim, output_dim],\n",
    "                                                        name='self_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.neigh_input_dim = neigh_input_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "        neigh_h = neigh_vecs\n",
    "\n",
    "        dims = tf.shape(neigh_h)\n",
    "        batch_size = dims[0]\n",
    "        num_neighbors = dims[1]\n",
    "        # [nodes * sampled neighbors] x [hidden_dim]\n",
    "        h_reshaped = tf.reshape(neigh_h, (batch_size * num_neighbors, self.neigh_input_dim))\n",
    "\n",
    "        for l in self.mlp_layers:\n",
    "            h_reshaped = l(h_reshaped)\n",
    "        neigh_h = tf.reshape(h_reshaped, (batch_size, num_neighbors, self.hidden_dim))\n",
    "        neigh_h = tf.reduce_max(neigh_h, axis=1)\n",
    "        \n",
    "        from_neighs = tf.matmul(neigh_h, self.vars['neigh_weights'])\n",
    "        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n",
    "        \n",
    "        if not self.concat:\n",
    "            output = tf.add_n([from_self, from_neighs])\n",
    "        else:\n",
    "            output = tf.concat([from_self, from_neighs], axis=1)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "       \n",
    "        return self.act(output)\n",
    "\n",
    "\n",
    "class MeanPoolingAggregator(Layer):\n",
    "    \"\"\" Aggregates via mean-pooling over MLP functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, model_size=\"small\", neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, name=None, concat=False, **kwargs):\n",
    "        super(MeanPoolingAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        if model_size == \"small\":\n",
    "            hidden_dim = self.hidden_dim = 512\n",
    "        elif model_size == \"big\":\n",
    "            hidden_dim = self.hidden_dim = 1024\n",
    "\n",
    "        self.mlp_layers = []\n",
    "        self.mlp_layers.append(Dense(input_dim=neigh_input_dim,\n",
    "                                 output_dim=hidden_dim,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=dropout,\n",
    "                                 sparse_inputs=False,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['neigh_weights'] = glorot([hidden_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "           \n",
    "            self.vars['self_weights'] = glorot([input_dim, output_dim],\n",
    "                                                        name='self_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.neigh_input_dim = neigh_input_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "        neigh_h = neigh_vecs\n",
    "\n",
    "        dims = tf.shape(neigh_h)\n",
    "        batch_size = dims[0]\n",
    "        num_neighbors = dims[1]\n",
    "        # [nodes * sampled neighbors] x [hidden_dim]\n",
    "        h_reshaped = tf.reshape(neigh_h, (batch_size * num_neighbors, self.neigh_input_dim))\n",
    "\n",
    "        for l in self.mlp_layers:\n",
    "            h_reshaped = l(h_reshaped)\n",
    "        neigh_h = tf.reshape(h_reshaped, (batch_size, num_neighbors, self.hidden_dim))\n",
    "        neigh_h = tf.reduce_mean(neigh_h, axis=1)\n",
    "        \n",
    "        from_neighs = tf.matmul(neigh_h, self.vars['neigh_weights'])\n",
    "        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n",
    "        \n",
    "        if not self.concat:\n",
    "            output = tf.add_n([from_self, from_neighs])\n",
    "        else:\n",
    "            output = tf.concat([from_self, from_neighs], axis=1)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "       \n",
    "        return self.act(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sOI-qG_4W3jM"
   },
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ouippyiXW0um"
   },
   "outputs": [],
   "source": [
    "class DGRec(object):\n",
    "\n",
    "    def __init__(self, args, support_sizes, placeholders):\n",
    "        self.support_sizes = support_sizes\n",
    "        if args.aggregator_type == \"mean\":\n",
    "            self.aggregator_cls = MeanAggregator\n",
    "        elif args.aggregator_type == \"seq\":\n",
    "            self.aggregator_cls = SeqAggregator\n",
    "        elif args.aggregator_type == \"maxpool\":\n",
    "            self.aggregator_cls = MaxPoolingAggregator\n",
    "        elif args.aggregator_type == \"meanpool\":\n",
    "            self.aggregator_cls = MeanPoolingAggregator\n",
    "        elif args.aggregator_type == \"gcn\":\n",
    "            self.aggregator_cls = GCNAggregator\n",
    "        elif args.aggregator_type == \"attn\":\n",
    "            self.aggregator_cls = AttentionAggregator\n",
    "        else:\n",
    "            raise Exception(\"Unknown aggregator: \", self.aggregator_cls)\n",
    "        self.input_x = placeholders['input_x']\n",
    "        self.input_y = placeholders['input_y']\n",
    "        self.mask_y = placeholders['mask_y']\n",
    "        self.mask = tf.cast(self.mask_y, dtype=tf.float32)\n",
    "        self.point_count = tf.reduce_sum(self.mask)\n",
    "        self.support_nodes_layer1 = placeholders['support_nodes_layer1']\n",
    "        self.support_nodes_layer2 = placeholders['support_nodes_layer2']\n",
    "        self.support_sessions_layer1 = placeholders['support_sessions_layer1']\n",
    "        self.support_sessions_layer2 = placeholders['support_sessions_layer2']\n",
    "        self.support_lengths_layer1 = placeholders['support_lengths_layer1']\n",
    "        self.support_lengths_layer2 = placeholders['support_lengths_layer2']\n",
    "\n",
    "        self.training = args.training\n",
    "        self.concat = args.concat\n",
    "        if args.act == 'linear':\n",
    "            self.act = lambda x:x\n",
    "        elif args.act == 'relu':\n",
    "            self.act = tf.nn.relu\n",
    "        elif args.act == 'elu':\n",
    "            self.act = tf.nn.elu\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.batch_size = args.batch_size\n",
    "        self.hidden_size = args.hidden_size\n",
    "        self.samples_1 = args.samples_1\n",
    "        self.samples_2 = args.samples_2\n",
    "        self.num_samples = [self.samples_1, self.samples_2]\n",
    "        self.n_items = args.num_items\n",
    "        self.n_users = args.num_users\n",
    "        self.emb_item = args.embedding_size\n",
    "        self.emb_user = args.emb_user\n",
    "        self.max_length = args.max_length\n",
    "        self.model_size = args.model_size\n",
    "        self.dropout = args.dropout\n",
    "        self.dim1 = args.dim1\n",
    "        self.dim2 = args.dim2\n",
    "        self.weight_decay = args.weight_decay\n",
    "        self.global_only = args.global_only\n",
    "        self.local_only = args.local_only\n",
    "\n",
    "        self.dims = [self.hidden_size, args.dim1, args.dim2]\n",
    "        self.dense_layers = []\n",
    "        self.loss = 0\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self.lr = tf.maximum(1e-5, tf.train.exponential_decay(args.learning_rate,\n",
    "                                                            self.global_step,\n",
    "                                                            args.decay_steps,\n",
    "                                                            args.decay_rate,\n",
    "                                                            staircase=True))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "        self.build()\n",
    "\n",
    "    def global_features(self):\n",
    "        self.user_embedding = tf.get_variable('user_embedding', [self.n_users, self.emb_user],\\\n",
    "                                        initializer=tf.glorot_uniform_initializer())\n",
    "        feature_layer1 = tf.nn.embedding_lookup(self.user_embedding, self.support_nodes_layer1)\n",
    "        feature_layer2 = tf.nn.embedding_lookup(self.user_embedding, self.support_nodes_layer2)\n",
    "        dense_layer = Dense(self.emb_user, \n",
    "                            self.hidden_size if self.global_only else self.hidden_size // 2,\n",
    "                            act=tf.nn.relu,\n",
    "                            dropout=self.dropout if self.training else 0.)\n",
    "        self.dense_layers.append(dense_layer)\n",
    "        feature_layer1 = dense_layer(feature_layer1)\n",
    "        feature_layer2 = dense_layer(feature_layer2)\n",
    "        return [feature_layer2, feature_layer1]\n",
    "    \n",
    "    def local_features(self):\n",
    "        '''\n",
    "        Use the same rnn in decode function\n",
    "        '''\n",
    "        initial_state_layer1 = self.lstm_cell.zero_state(self.batch_size*self.samples_1*self.samples_2, dtype=tf.float32)\n",
    "        initial_state_layer2 = self.lstm_cell.zero_state(self.batch_size*self.samples_2, dtype=tf.float32)\n",
    "        inputs_1 = tf.nn.embedding_lookup(self.embedding, self.support_sessions_layer1)\n",
    "        inputs_2 = tf.nn.embedding_lookup(self.embedding, self.support_sessions_layer2)\n",
    "        outputs1, states1 = tf.nn.dynamic_rnn(cell=self.lstm_cell,\n",
    "                                            inputs=inputs_1, \n",
    "                                            sequence_length=self.support_lengths_layer1,\n",
    "                                            initial_state=initial_state_layer1,\n",
    "                                            dtype=tf.float32)\n",
    "        outputs2, states2 = tf.nn.dynamic_rnn(cell=self.lstm_cell,\n",
    "                                            inputs=inputs_2, \n",
    "                                            sequence_length=self.support_lengths_layer2,\n",
    "                                            initial_state=initial_state_layer2,\n",
    "                                            dtype=tf.float32)\n",
    "        # outputs: shape[batch_size, max_time, depth]\n",
    "        local_layer1 = states1.h\n",
    "        local_layer2 = states2.h\n",
    "        dense_layer = Dense(self.hidden_size, \n",
    "                            self.hidden_size if self.local_only else self.hidden_size // 2,\n",
    "                            act=tf.nn.relu,\n",
    "                            dropout=self.dropout if self.training else 0.)\n",
    "        self.dense_layers.append(dense_layer)\n",
    "        local_layer1 = dense_layer(local_layer1)\n",
    "        local_layer2 = dense_layer(local_layer2)\n",
    "        return [local_layer2, local_layer1]\n",
    "\n",
    "    def global_and_local_features(self):\n",
    "        #global features\n",
    "        global_feature_layer2, global_feature_layer1 = self.global_features()\n",
    "        local_feature_layer2, local_feature_layer1 = self.local_features()\n",
    "        global_local_layer2 = tf.concat([global_feature_layer2, local_feature_layer2], -1)\n",
    "        global_local_layer1 = tf.concat([global_feature_layer1, local_feature_layer1], -1)\n",
    "        return [global_local_layer2, global_local_layer1]\n",
    "\n",
    "    def aggregate(self, hidden, dims, num_samples, support_sizes, \n",
    "            aggregators=None, name=None, concat=False, model_size=\"small\"):\n",
    "        \"\"\" At each layer, aggregate hidden representations of neighbors to compute the hidden representations \n",
    "            at next layer.\n",
    "        Args:\n",
    "            samples: a list of samples of variable hops away for convolving at each layer of the\n",
    "                network. Length is the number of layers + 1. Each is a vector of node indices.\n",
    "            input_features: the input features for each sample of various hops away.\n",
    "            dims: a list of dimensions of the hidden representations from the input layer to the\n",
    "                final layer. Length is the number of layers + 1.\n",
    "            num_samples: list of number of samples for each layer.\n",
    "            support_sizes: the number of nodes to gather information from for each layer.\n",
    "            batch_size: the number of inputs (different for batch inputs and negative samples).\n",
    "        Returns:\n",
    "            The hidden representation at the final layer for all nodes in batch\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # length: number of layers + 1\n",
    "        hidden = hidden\n",
    "        new_agg = aggregators is None\n",
    "        if new_agg:\n",
    "            aggregators = []\n",
    "        for layer in range(len(num_samples)):\n",
    "            if new_agg:\n",
    "                dim_mult = 2 if concat and (layer != 0) else 1\n",
    "                # aggregator at current layer\n",
    "                if layer == len(num_samples) - 1:\n",
    "                    aggregator = self.aggregator_cls(dim_mult*dims[layer], dims[layer+1], act=lambda x : x,\n",
    "                            dropout=self.dropout if self.training else 0., \n",
    "                            name=name, concat=concat, model_size=model_size)\n",
    "                else:\n",
    "                    aggregator = self.aggregator_cls(dim_mult*dims[layer], dims[layer+1], act=self.act,\n",
    "                            dropout=self.dropout if self.training else 0., \n",
    "                            name=name, concat=concat, model_size=model_size)\n",
    "                aggregators.append(aggregator)\n",
    "            else:\n",
    "                aggregator = aggregators[layer]\n",
    "            # hidden representation at current layer for all support nodes that are various hops away\n",
    "            next_hidden = []\n",
    "            # as layer increases, the number of support nodes needed decreases\n",
    "            for hop in range(len(num_samples) - layer):\n",
    "                dim_mult = 2 if concat and (layer != 0) else 1\n",
    "                neigh_dims = [self.batch_size * support_sizes[hop], \n",
    "                              num_samples[len(num_samples) - hop - 1], \n",
    "                              dim_mult*dims[layer]]\n",
    "                h = aggregator((hidden[hop],\n",
    "                                tf.reshape(hidden[hop + 1], neigh_dims)))\n",
    "                next_hidden.append(h)\n",
    "            hidden = next_hidden\n",
    "        return hidden[0], aggregators\n",
    "\n",
    "    def decode(self):\n",
    "        self.lstm_cell = lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self.hidden_size)\n",
    "        initial_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        time_major_x = tf.transpose(self.input_x)\n",
    "        inputs = tf.nn.embedding_lookup(self.embedding, time_major_x)\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                            inputs=inputs, \n",
    "                                            initial_state=initial_state,\n",
    "                                            time_major=True,\n",
    "                                            dtype=tf.float32,\n",
    "                                            scope='decode_rnn')\n",
    "        # outputs: shape[max_time, batch_size, depth]\n",
    "        slices = tf.split(outputs, num_or_size_splits=self.max_length, axis=0)\n",
    "        return [tf.squeeze(t,[0]) for t in slices]\n",
    "\n",
    "    def step_by_step(self, features_0, features_1_2, dims, num_samples, support_sizes, \n",
    "            aggregators=None, name=None, concat=False, model_size=\"small\"):\n",
    "        self.aggregators = None\n",
    "        outputs = []\n",
    "        for feature0 in features_0:\n",
    "            hidden = [feature0, features_1_2[0], features_1_2[1]]\n",
    "            output1, self.aggregators = self.aggregate(hidden, dims, num_samples, support_sizes,\n",
    "                                        aggregators=self.aggregators, concat=concat, model_size=self.model_size)\n",
    "            outputs.append(output1)\n",
    "        return tf.stack(outputs, axis=0)\n",
    "\n",
    "    def build(self):\n",
    "        self.embedding = embedding = tf.get_variable('item_embedding', [self.n_items, self.emb_item],\\\n",
    "                                        initializer=tf.glorot_uniform_initializer())\n",
    "        features_0 = self.decode() # features of zero layer nodes. \n",
    "        #outputs with shape [max_time, batch_size, dim2]\n",
    "        if self.global_only:\n",
    "            features_1_2 = self.global_features()\n",
    "        elif self.local_only:\n",
    "            features_1_2 = self.local_features()\n",
    "        else:\n",
    "            features_1_2 = self.global_and_local_features()\n",
    "        outputs = self.step_by_step(features_0, features_1_2, self.dims, self.num_samples, self.support_sizes,\n",
    "                                concat=self.concat)\n",
    "        concat_self = tf.concat([features_0, outputs], axis=-1)\n",
    "\n",
    "        # exchange first two dimensions.\n",
    "        self.transposed_outputs = tf.transpose(concat_self, [1,0,2])\n",
    "\n",
    "        self.loss = self._loss()\n",
    "        self.sum_recall = self._recall()\n",
    "        self.sum_ndcg = self._ndcg()\n",
    "        grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "        clipped_grads_and_vars = [(tf.clip_by_value(grad, -5.0, 5.0) if grad is not None else None, var)\n",
    "                        for grad, var in grads_and_vars]\n",
    "        self.opt_op = self.optimizer.apply_gradients(clipped_grads_and_vars, global_step=self.global_step)\n",
    "    \n",
    "    def _loss(self):\n",
    "        reg_loss = 0.\n",
    "        xe_loss = 0.\n",
    "        fc_layer = Dense(self.dim2 + self.hidden_size, self.emb_item, act=lambda x:x, dropout=self.dropout if self.training else 0.)\n",
    "        self.dense_layers.append(fc_layer)\n",
    "        self.logits = logits = tf.matmul(fc_layer(tf.reshape(self.transposed_outputs, [-1, self.dim2+self.hidden_size])), self.embedding, transpose_b=True)\n",
    "        for dense_layer in self.dense_layers:\n",
    "            for var in dense_layer.vars.values():\n",
    "                reg_loss += self.weight_decay * tf.nn.l2_loss(var)\n",
    "        for aggregator in self.aggregators:\n",
    "            for var in aggregator.vars.values():\n",
    "                reg_loss += self.weight_decay * tf.nn.l2_loss(var)\n",
    "        reshaped_logits = tf.reshape(logits, [self.batch_size, self.max_length, self.n_items])\n",
    "        xe_loss += tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y,\n",
    "                                                            logits=reshaped_logits,\n",
    "                                                            name='softmax_loss')\n",
    "        xe_loss *= self.mask\n",
    "        return tf.reduce_sum(xe_loss) / self.point_count + reg_loss\n",
    "\n",
    "    def _ndcg(self):\n",
    "        predictions = tf.transpose(self.logits)\n",
    "        targets = tf.reshape(self.input_y, [-1])\n",
    "        pred_values = tf.expand_dims(tf.diag_part(tf.nn.embedding_lookup(predictions, targets)), -1)\n",
    "        tile_pred_values = tf.tile(pred_values, [1, self.n_items-1])\n",
    "        ranks = tf.reduce_sum(tf.cast(self.logits[:,1:] > tile_pred_values, dtype=tf.float32), -1) + 1\n",
    "        ndcg = 1. / (log2(1.0 + ranks))\n",
    "        mask = tf.reshape(self.mask, [-1])\n",
    "        ndcg *= mask\n",
    "        return tf.reduce_sum(ndcg)\n",
    "\n",
    "    def _recall(self):\n",
    "        predictions = self.logits\n",
    "        targets = tf.reshape(self.input_y, [-1])\n",
    "        recall_at_k = tf.nn.in_top_k(predictions, targets, k=20)\n",
    "        recall_at_k = tf.cast(recall_at_k, dtype=tf.float32)\n",
    "        mask = tf.reshape(self.mask, [-1])\n",
    "        recall_at_k *= mask\n",
    "        return tf.reduce_sum(recall_at_k)\n",
    "\n",
    "def log2(x):\n",
    "    numerator = tf.log(x)\n",
    "    denominator = tf.log(tf.constant(2, dtype=numerator.dtype))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VsATTYvvXAFg"
   },
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "colab_type": "code",
    "id": "atnLGsfBW9Vu",
    "outputId": "72caf72b-339d-4a48-eb57-09157264670a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data..\n",
      "Training data loaded!\n",
      "sessions: 613221\tratings: 2554567\n",
      "sessions: 7354\tratings: 27185\n",
      "sessions: 7304\tratings: 26705\n",
      "WARNING:tensorflow:From <ipython-input-30-99f58a3dad2a>:175: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-30-99f58a3dad2a>:184: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From <ipython-input-26-ff7ddbaadb98>:100: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From <ipython-input-29-cdde2cc94bd2>:157: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "Epoch: 0001\n",
      "Iter: 0 val_loss= 9.44080 val_recall@20= 0.00156 val_ndcg= 0.08456 dump model!\n",
      "Iter: 0 train_loss= 9.44082 train_recall@20= 0.00173 train_ndcg= 0.08428 val_loss= 9.44080 val_recall@20= 0.00156 val_ndcg= 0.08456 time= 7.16685s\n",
      "Iter: 100 train_loss= 8.87542 train_recall@20= 0.02638 train_ndcg= 0.11023 val_loss= 9.44080 val_recall@20= 0.00156 val_ndcg= 0.08456 time= 3.73285s\n",
      "Iter: 200 train_loss= 8.70583 train_recall@20= 0.02898 train_ndcg= 0.11250 val_loss= 9.44080 val_recall@20= 0.00156 val_ndcg= 0.08456 time= 3.72444s\n",
      "Iter: 300 train_loss= 8.64277 train_recall@20= 0.03012 train_ndcg= 0.11334 val_loss= 9.44080 val_recall@20= 0.00156 val_ndcg= 0.08456 time= 3.73193s\n",
      "Iter: 400 train_loss= 8.60544 train_recall@20= 0.03143 train_ndcg= 0.11406 val_loss= 9.44080 val_recall@20= 0.00156 val_ndcg= 0.08456 time= 3.74075s\n",
      "Iter: 500 val_loss= 8.37362 val_recall@20= 0.03420 val_ndcg= 0.11762 dump model!\n",
      "Iter: 500 train_loss= 8.56750 train_recall@20= 0.03349 train_ndcg= 0.11519 val_loss= 8.37362 val_recall@20= 0.03420 val_ndcg= 0.11762 time= 3.74051s\n",
      "Iter: 600 train_loss= 8.51733 train_recall@20= 0.03671 train_ndcg= 0.11699 val_loss= 8.37362 val_recall@20= 0.03420 val_ndcg= 0.11762 time= 3.74549s\n",
      "Iter: 700 train_loss= 8.46886 train_recall@20= 0.04001 train_ndcg= 0.11867 val_loss= 8.37362 val_recall@20= 0.03420 val_ndcg= 0.11762 time= 3.74661s\n",
      "Iter: 800 train_loss= 8.42391 train_recall@20= 0.04320 train_ndcg= 0.12033 val_loss= 8.37362 val_recall@20= 0.03420 val_ndcg= 0.11762 time= 3.74520s\n",
      "Iter: 900 train_loss= 8.38257 train_recall@20= 0.04645 train_ndcg= 0.12190 val_loss= 8.37362 val_recall@20= 0.03420 val_ndcg= 0.11762 time= 3.74365s\n",
      "Iter: 1000 val_loss= 8.15748 val_recall@20= 0.04849 val_ndcg= 0.12748 dump model!\n",
      "Iter: 1000 train_loss= 8.34339 train_recall@20= 0.04974 train_ndcg= 0.12345 val_loss= 8.15748 val_recall@20= 0.04849 val_ndcg= 0.12748 time= 3.74235s\n",
      "Iter: 1100 train_loss= 8.30361 train_recall@20= 0.05306 train_ndcg= 0.12506 val_loss= 8.15748 val_recall@20= 0.04849 val_ndcg= 0.12748 time= 3.73954s\n"
     ]
    }
   ],
   "source": [
    "#coding=utf-8\n",
    "\n",
    "import os, sys\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "def evaluate(sess, model, minibatch, val_or_test='val'):\n",
    "    epoch_val_cost = []\n",
    "    epoch_val_recall = []\n",
    "    epoch_val_ndcg = []\n",
    "    epoch_val_point = []\n",
    "    while not minibatch.end_val(val_or_test):\n",
    "        feed_dict = minibatch.next_val_minibatch_feed_dict(val_or_test)\n",
    "        outs = sess.run([model.loss,model.sum_recall, model.sum_ndcg, model.point_count], feed_dict=feed_dict)\n",
    "        epoch_val_cost.append(outs[0])\n",
    "        epoch_val_recall.append(outs[1])\n",
    "        epoch_val_ndcg.append(outs[2])\n",
    "        epoch_val_point.append(outs[3])\n",
    "    return np.mean(epoch_val_cost), np.sum(epoch_val_recall) / np.sum(epoch_val_point), np.sum(epoch_val_ndcg) / np.sum(epoch_val_point)\n",
    "\n",
    "def construct_placeholders(args):\n",
    "    # Define placeholders\n",
    "    placeholders = {\n",
    "        'input_x': tf.placeholder(tf.int32, shape=(args.batch_size, args.max_length), name='input_session'),\n",
    "        'input_y': tf.placeholder(tf.int32, shape=(args.batch_size, args.max_length), name='output_session'),\n",
    "        'mask_y': tf.placeholder(tf.float32, shape=(args.batch_size, args.max_length), name='mask_x'),\n",
    "        'support_nodes_layer1': tf.placeholder(tf.int32, shape=(args.batch_size*args.samples_1*args.samples_2), name='support_nodes_layer1'),\n",
    "        'support_nodes_layer2': tf.placeholder(tf.int32, shape=(args.batch_size*args.samples_2), name='support_nodes_layer2'),\n",
    "        'support_sessions_layer1': tf.placeholder(tf.int32, shape=(args.batch_size*args.samples_1*args.samples_2,\\\n",
    "                                    args.max_length), name='support_sessions_layer1'),\n",
    "        'support_sessions_layer2': tf.placeholder(tf.int32, shape=(args.batch_size*args.samples_2,\\\n",
    "                                    args.max_length), name='support_sessions_layer2'),\n",
    "        'support_lengths_layer1': tf.placeholder(tf.int32, shape=(args.batch_size*args.samples_1*args.samples_2), \n",
    "                                    name='support_lengths_layer1'),\n",
    "        'support_lengths_layer2': tf.placeholder(tf.int32, shape=(args.batch_size*args.samples_2), \n",
    "                                    name='support_lengths_layer2'),\n",
    "    }\n",
    "    return placeholders\n",
    "\n",
    "def train(args, data):\n",
    "    adj_info = data[0]\n",
    "    latest_per_user_by_time = data[1]\n",
    "    user_id_map = data[2]\n",
    "    item_id_map = data[3]\n",
    "    train_df = data[4]\n",
    "    valid_df = data[5]\n",
    "    test_df = data[6]\n",
    "    \n",
    "    args.num_items = len(item_id_map) + 1\n",
    "    args.num_users = len(user_id_map)\n",
    "    placeholders = construct_placeholders(args)\n",
    "    if not os.path.exists(args.ckpt_dir):\n",
    "        os.makedirs(args.ckpt_dir)\n",
    "    ckpt_path = os.path.join(args.ckpt_dir, 'model.ckpt')\n",
    "\n",
    "    minibatch = MinibatchIterator(adj_info,\n",
    "                latest_per_user_by_time,\n",
    "                [train_df, valid_df, test_df],\n",
    "                placeholders,\n",
    "                batch_size=args.batch_size,\n",
    "                max_degree=args.max_degree,\n",
    "                num_nodes=len(user_id_map),\n",
    "                max_length=args.max_length,\n",
    "                samples_1_2=[args.samples_1, args.samples_2])\n",
    "    \n",
    "    dgrec = DGRec(args, minibatch.sizes, placeholders)\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=3)\n",
    "\n",
    "    total_steps = 0\n",
    "    avg_time = 0.\n",
    "\n",
    "    patience = 10\n",
    "    inc = 0\n",
    "    early_stopping = False\n",
    "\n",
    "    highest_val_recall = -1.0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(args.epochs):\n",
    "        minibatch.shuffle()\n",
    "\n",
    "        iter_cn = 0\n",
    "        print('Epoch: %04d' % (epoch + 1))\n",
    "        epoch_val_cost = []\n",
    "        epoch_val_recall = []\n",
    "        epoch_val_ndcg = []\n",
    "        epoch_train_cost = []\n",
    "        epoch_train_recall = []\n",
    "        epoch_train_ndcg = []\n",
    "        epoch_train_point = []\n",
    "        \n",
    "        while not minibatch.end() and not early_stopping:\n",
    "            t = time.time()\n",
    "            feed_dict = minibatch.next_train_minibatch_feed_dict()\n",
    "            outs = sess.run([dgrec.opt_op, dgrec.loss, dgrec.sum_recall, dgrec.sum_ndcg, dgrec.point_count], feed_dict=feed_dict)\n",
    "            train_cost = outs[1]\n",
    "            epoch_train_cost.append(train_cost)\n",
    "            epoch_train_recall.append(outs[2])\n",
    "            epoch_train_ndcg.append(outs[3])\n",
    "            epoch_train_point.append(outs[4])\n",
    "            # Print results\n",
    "            avg_time = (avg_time * total_steps + time.time() - t) / (total_steps + 1)\n",
    "\n",
    "            if iter_cn % args.val_every == 0:\n",
    "                ret = evaluate(sess, dgrec, minibatch)\n",
    "                epoch_val_cost.append(ret[0])\n",
    "                epoch_val_recall.append(ret[1])\n",
    "                epoch_val_ndcg.append(ret[2])\n",
    "                if ret[1] >= highest_val_recall:\n",
    "                    saver.save(sess, ckpt_path, global_step=total_steps)\n",
    "                    highest_val_recall = ret[1]\n",
    "                    inc = 0\n",
    "                    print(\"Iter:\", '%d' % iter_cn, \n",
    "                          \"val_loss=\", \"{:.5f}\".format(epoch_val_cost[-1]),\n",
    "                          \"val_recall@20=\", \"{:.5f}\".format(epoch_val_recall[-1]),\n",
    "                          \"val_ndcg=\", \"{:.5f}\".format(epoch_val_ndcg[-1]),\n",
    "                          \"dump model!\"\n",
    "                          )\n",
    "                else:\n",
    "                    inc += 1\n",
    "                if inc >= patience:\n",
    "                    early_stopping = True\n",
    "                    break\n",
    "\n",
    "            if total_steps % args.print_every == 0:\n",
    "                print(\"Iter:\", '%d' % iter_cn, \n",
    "                      \"train_loss=\", \"{:.5f}\".format(np.mean(epoch_train_cost)),\n",
    "                      \"train_recall@20=\", \"{:.5f}\".format(np.sum(epoch_train_recall)/np.sum(epoch_train_point)),\n",
    "                      \"train_ndcg=\", \"{:.5f}\".format(np.sum(epoch_train_ndcg)/np.sum(epoch_train_point)),\n",
    "                      \"val_loss=\", \"{:.5f}\".format(epoch_val_cost[-1]),\n",
    "                      \"val_recall@20=\", \"{:.5f}\".format(epoch_val_recall[-1]),\n",
    "                      \"val_ndcg=\", \"{:.5f}\".format(epoch_val_ndcg[-1]),\n",
    "                      \"time=\", \"{:.5f}s\".format(avg_time))\n",
    "                sys.stdout.flush()\n",
    "            total_steps += 1\n",
    "            iter_cn += 1\n",
    "        if early_stopping:\n",
    "            print('Early stop at epoch: {}, total training steps: {}'.format(epoch, total_steps))\n",
    "            break\n",
    "    end_time = time.time() \n",
    "    print('-----------{} seconds per batch iteration-------------'.format((end_time - start_time) / total_steps))\n",
    "    print('Parameter settings: {}'.format(args.ckpt_dir))\n",
    "    print('Optimization finished!\\tStart testing...')\n",
    "    ret = evaluate(sess, dgrec, minibatch, 'test')\n",
    "    print('Test results:',\n",
    "            '\\tLoss:{}'.format(ret[0]),\n",
    "            '\\tRecall@20:{}'.format(ret[1]),\n",
    "            '\\tNDCG:{}'.format(ret[2]))\n",
    "    \n",
    "class Args():\n",
    "  training = True\n",
    "  global_only = False\n",
    "  local_only = False\n",
    "  epochs = 20\n",
    "  aggregator_type='attn'\n",
    "  act='relu'\n",
    "  batch_size = 200\n",
    "  max_degree = 50\n",
    "  num_users = -1\n",
    "  num_items = 100\n",
    "  concat=False\n",
    "  learning_rate=0.001\n",
    "  hidden_size = 100\n",
    "  embedding_size = 50\n",
    "  emb_user = 50\n",
    "  max_length=20\n",
    "  samples_1=10\n",
    "  samples_2=5\n",
    "  dim1 = 100\n",
    "  dim2 = 100\n",
    "  model_size = 'small'\n",
    "  dropout = 0.\n",
    "  weight_decay = 0.\n",
    "  decay_steps = 400\n",
    "  decay_rate = 0.98\n",
    "  print_every = 100\n",
    "  val_every = 500\n",
    "  ckpt_dir = 'save/'\n",
    "\n",
    "args = Args()\n",
    "print('Loading training data..')\n",
    "data = load_data('/content')\n",
    "print(\"Training data loaded!\")\n",
    "train(args, data)\n",
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EqXl2q-Vam7I"
   },
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vzEQg8HUXQHD"
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "\n",
    "def evaluate(sess, model, minibatch, val_or_test='val'):\n",
    "    epoch_val_cost = []\n",
    "    epoch_val_recall = []\n",
    "    epoch_val_ndcg = []\n",
    "    epoch_val_point = []\n",
    "    input_str = []\n",
    "    while not minibatch.end_val(val_or_test):\n",
    "        feed_dict = minibatch.next_val_minibatch_feed_dict(val_or_test)\n",
    "        x = np.reshape(feed_dict[minibatch.placeholders['input_x']], -1).tolist()\n",
    "        x_str = '_'.join([str(v) for v in x if v !=0])\n",
    "        input_str.append(x_str)\n",
    "        outs = sess.run([model.loss,model.sum_recall, model.sum_ndcg, model.point_count], feed_dict=feed_dict)\n",
    "        epoch_val_cost.append(outs[0])\n",
    "        epoch_val_recall.append(outs[1])\n",
    "        epoch_val_ndcg.append(outs[2])\n",
    "        epoch_val_point.append(outs[3])\n",
    "    return [np.mean(epoch_val_cost), np.sum(epoch_val_recall) / np.sum(epoch_val_point), np.sum(epoch_val_ndcg) / np.sum(epoch_val_point), epoch_val_recall, epoch_val_ndcg, input_str]\n",
    "\n",
    "def construct_placeholders(args):\n",
    "    # Define placeholders\n",
    "    placeholders = {\n",
    "        'input_x': tf.placeholder(tf.int32, shape=(args.batch_size, args.max_length), name='input_session'),\n",
    "        'input_y': tf.placeholder(tf.int32, shape=(args.batch_size, args.max_length), name='output_session'),\n",
    "        'mask_y': tf.placeholder(tf.float32, shape=(args.batch_size, args.max_length), name='mask_x'),\n",
    "        'support_nodes_layer1': tf.placeholder(tf.int32, shape=(args.batch_size*args.samples_1*args.samples_2), name='support_nodes_layer1'),\n",
    "        'support_nodes_layer2': tf.placeholder(tf.int32, shape=(args.batch_size*args.samples_2), name='support_nodes_layer2'),\n",
    "        'support_sessions_layer1': tf.placeholder(tf.int32, shape=(args.batch_size*args.samples_1*args.samples_2,\\\n",
    "                                    args.max_length), name='support_sessions_layer1'),\n",
    "        'support_sessions_layer2': tf.placeholder(tf.int32, shape=(args.batch_size*args.samples_2,\\\n",
    "                                    args.max_length), name='support_sessions_layer2'),\n",
    "        'support_lengths_layer1': tf.placeholder(tf.int32, shape=(args.batch_size*args.samples_1*args.samples_2), \n",
    "                                    name='support_lengths_layer1'),\n",
    "        'support_lengths_layer2': tf.placeholder(tf.int32, shape=(args.batch_size*args.samples_2), \n",
    "                                    name='support_lengths_layer2'),\n",
    "    }\n",
    "    return placeholders\n",
    "\n",
    "def test(args, data):\n",
    "    adj_info = data[0]\n",
    "    latest_per_user_by_time = data[1]\n",
    "    user_id_map = data[2]\n",
    "    item_id_map = data[3]\n",
    "    train_df = data[4]\n",
    "    valid_df = data[5]\n",
    "    test_df = data[6]\n",
    "    \n",
    "    args.num_items = len(item_id_map) + 1\n",
    "    args.num_users = len(user_id_map)\n",
    "    args.batch_size = 1\n",
    "    placeholders = construct_placeholders(args)\n",
    "    \n",
    "    minibatch = MinibatchIterator(adj_info,\n",
    "                latest_per_user_by_time,\n",
    "                [train_df, valid_df, test_df],\n",
    "                placeholders,\n",
    "                batch_size=args.batch_size,\n",
    "                max_degree=args.max_degree,\n",
    "                num_nodes=len(user_id_map),\n",
    "                max_length=args.max_length,\n",
    "                samples_1_2=[args.samples_1, args.samples_2],\n",
    "                training=False)\n",
    "    \n",
    "    dgrec = DGRec(args, minibatch.sizes, placeholders)\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=3)\n",
    "    ckpt = tf.train.get_checkpoint_state(args.ckpt_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print('Restore model from {}!'.format(args.ckpt_dir))\n",
    "    else:\n",
    "        print('Failed to restore model from {}'.format(args.ckpt_dir))\n",
    "        sys.exit(0)\n",
    "    ret = evaluate(sess, dgrec, minibatch, \"test\")\n",
    "    print(\"Test results(batch_size=1):\",\n",
    "          \"\\tloss=\", \"{:.5f}\".format(ret[0]),\n",
    "          \"\\trecall@20=\", \"{:.5f}\".format(ret[1]),\n",
    "          \"\\tndcg=\", \"{:.5f}\".format(ret[2]),\n",
    "          )\n",
    "\n",
    "    recall = ret[-3]\n",
    "    ndcg = ret[-2]\n",
    "    x_strs = ret[-1]\n",
    "    with open('metric_dist.txt','w') as f:\n",
    "        for idx in range(len(ret[-1])):\n",
    "            f.write(x_strs[idx] + '\\t' + str(recall[idx]) + '\\t' + str(ndcg[idx]) + '\\n')\n",
    "\n",
    "class Args():\n",
    "    training = False\n",
    "    global_only = False\n",
    "    local_only = False\n",
    "    epochs = 20\n",
    "    aggregator_type='attn'\n",
    "    act='linear'\n",
    "    batch_size = 200\n",
    "    max_degree = 50\n",
    "    num_users = -1\n",
    "    num_items = 100\n",
    "    concat=False\n",
    "    learning_rate=0.001\n",
    "    hidden_size = 100\n",
    "    embedding_size = 100\n",
    "    emb_user = 100\n",
    "    max_length=20\n",
    "    samples_1=10\n",
    "    samples_2=5\n",
    "    dim1 = 100\n",
    "    dim2 = 100\n",
    "    model_size = 'small'\n",
    "    dropout = 0.\n",
    "    weight_decay = 0.\n",
    "    print_every = 100\n",
    "    val_every = 500\n",
    "    ckpt_dir = 'save/'\n",
    "\n",
    "args = Args()\n",
    "print('Loading data..')\n",
    "data = load_data('/content')\n",
    "print(\"Done loading data..\")\n",
    "test(args, data)\n",
    "\n",
    "tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPHn2J80Vn9O+jO+8pT7Xny",
   "collapsed_sections": [],
   "mount_file_id": "1Kiocy33z1iHy9eiRXdReEJ2m_31w-aFs",
   "name": "recsys_play_doubansocial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
