{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZzLBRl0iNfE"
   },
   "source": [
    "# Hands-on: movie recommender system\n",
    "## Collaborative filtering (matrix factorization)\n",
    "\n",
    "You are an online retailer/travel agent/movie review website, and you would like to help the visitors of your website to explore more of your products/destinations/movies. You got data which either describe the different products/destinations/films, or past transactions/trips/views (or preferences) of your visitors (or both!). You decide to leverage that data to provide relevant and meaningful recommendations.\n",
    "\n",
    "This notebook implements a simple collaborative system using  factorization of the user-item matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8u8ZXvkhkfY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPVCIslerd5e"
   },
   "outputs": [],
   "source": [
    "ratings=\"https://github.com/couturierc/tutorials/raw/master/recommender_system/data/ratings.csv\"\n",
    "movies=\"https://github.com/couturierc/tutorials/raw/master/recommender_system/data/movies.csv\"\n",
    "\n",
    "# If data stored locally\n",
    "# ratings=\"./data/ratings.csv\"\n",
    "# movies=\"./data/movies.csv\"\n",
    "\n",
    "df_ratings = pd.read_csv(ratings, sep=',')\n",
    "df_ratings.columns = ['userId', 'itemId', 'rating', 'timestamp']\n",
    "df_movies = pd.read_csv(movies, sep=',')\n",
    "df_movies.columns = ['itemId', 'title', 'genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvyAYay5rzcS"
   },
   "outputs": [],
   "source": [
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4K08KX3sYhr"
   },
   "outputs": [],
   "source": [
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hUKyFxYdsT5"
   },
   "source": [
    "## Quick exploration\n",
    "\n",
    "Hints: use df.describe(), df.column_name.hist(), scatterplot matrix (sns.pairplot(df[column_range])), correlation matrix (sns.heatmap(df.corr()) ), check duplicates, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVqBtDNmJ5vL"
   },
   "outputs": [],
   "source": [
    "# Start your exploration -- use as many cells as you need !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MffuKcE5s8fQ"
   },
   "source": [
    "## Obtain the user-item matrice by pivoting df_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOt3GI3zs2Ts"
   },
   "outputs": [],
   "source": [
    "##### FILL HERE (1 line) ######\n",
    "df_user_item = NULL # Use df.pivot, rows ~ userId's, columns ~ itemId's\n",
    "################################\n",
    "\n",
    "# Sort index/rows (userId's) and columns (itemId's)\n",
    "df_user_item.sort_index(axis=0, inplace=True)\n",
    "df_user_item.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90Q7L3SQtc1t"
   },
   "source": [
    "This matrix has **many** missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6tkf_s3tgsL"
   },
   "outputs": [],
   "source": [
    "df_user_item.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0EfDXLIRWaG"
   },
   "outputs": [],
   "source": [
    "df_user_item.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXanXrqI4xJ4"
   },
   "source": [
    "For instance, rating for userId=1 for movies with itemId 1 to 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLI0gnwT4obE"
   },
   "outputs": [],
   "source": [
    "df_user_item.loc[1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SM4RU3njy2K"
   },
   "outputs": [],
   "source": [
    "# df_user_item.loc[1].dropna().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dtJPkm1knNC"
   },
   "source": [
    "Save the movie ids for user 1 for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C05fKcNrkmYv"
   },
   "outputs": [],
   "source": [
    "item_rated_user_1 = df_user_item.loc[1].dropna().index\n",
    "item_rated_user_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR-pEwd5thyy"
   },
   "source": [
    "We want to find the matrix of rank $k$ which is closest to the original matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAUU_b5ma5bA"
   },
   "source": [
    "## What not to do: Fill with 0's or mean values, then Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ixiAfGIH6VU"
   },
   "source": [
    "(Adapted from https://github.com/beckernick/matrix_factorization_recommenders/blob/master/matrix_factorization_recommender.ipynb)\n",
    "\n",
    "Singular Value Decomposition decomposes a matrix $R$ into the best lower rank (i.e. smaller/simpler) approximation of the original matrix $R$. Mathematically, it decomposes R into a two unitary matrices and a diagonal matrix:\n",
    "\n",
    "$$\\begin{equation}\n",
    "R = U\\Sigma V^{T}\n",
    "\\end{equation}$$\n",
    "\n",
    "where: \n",
    "- R is users's ratings matrix, \n",
    "- $U$ is the user \"features\" matrix, it represents how much users \"like\" each feature,\n",
    "- $\\Sigma$ is the diagonal matrix of singular values (essentially weights), \n",
    "- $V^{T}$ is the movie \"features\" matrix, it represents how relevant each feature is to each movie,\n",
    "\n",
    "with $U$ and $V^{T}$ orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMVe_feVQQK_"
   },
   "outputs": [],
   "source": [
    "df_user_item = df_user_item.fillna(0)\n",
    "df_user_item.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pz16Rlw4tlom"
   },
   "outputs": [],
   "source": [
    "R = df_user_item.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_R9inUPkH1Hm"
   },
   "outputs": [],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gypFSYCYHg63"
   },
   "source": [
    "Apply SVD to R (e.g. using NumPy or SciPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGSFlWxLHYVE"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "U, sigma, Vt = svds(R, k = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slRJZ23uIVLt"
   },
   "source": [
    "What do $U$, $\\Sigma$, $V^T$ look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfifORX6IIga"
   },
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXkKnGWcISzH"
   },
   "outputs": [],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0H56AlQIUTM"
   },
   "outputs": [],
   "source": [
    "Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baQzWyVHKQVN"
   },
   "source": [
    "Get recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyzbchyIKnkW"
   },
   "outputs": [],
   "source": [
    "# First make sigma a diagonal matrix:\n",
    "sigma = np.diag(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uouELHsfKtOU"
   },
   "outputs": [],
   "source": [
    "R_after_svd = np.dot(np.dot(U, sigma), Vt)\n",
    "R_after_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFID_6eWKskb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6NRarPjJ0DI"
   },
   "source": [
    "Drawbacks of this approach: \n",
    "- the missing values (here filled with 0's) is feedback that the user did not give, we should not cannot consider it negative/null rating.\n",
    "- the dense matrix is huge, applying SVD is not scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Keb06kCFbIPl"
   },
   "source": [
    "## Approximate SVD with stochastic gradient descend (SGD)\n",
    "\n",
    "\n",
    "This time, we do **not** fill missing values. \n",
    "\n",
    "We inject $\\Sigma$ into U and V, and try to find P and q such that $\\widehat{R} = P Q^{T}$ is close to  $R$ **for the item-user pairs already rated**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkr8jfzbVS_R"
   },
   "source": [
    "A first function to simplify the entries (userId/itemId) : we map the set of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_HgEkPAQSTG"
   },
   "outputs": [],
   "source": [
    "def encode_ids(data):\n",
    "    '''Takes a rating dataframe and return: \n",
    "    - a simplified rating dataframe with ids in range(nb unique id) for users and movies\n",
    "    - 2 mapping disctionaries\n",
    "    \n",
    "    '''\n",
    "\n",
    "    data_encoded = data.copy()\n",
    "    \n",
    "    users = pd.DataFrame(data_encoded.userId.unique(),columns=['userId'])  # df of all unique users\n",
    "    dict_users = users.to_dict()    \n",
    "    inv_dict_users = {v: k for k, v in dict_users['userId'].items()}\n",
    "\n",
    "    items = pd.DataFrame(data_encoded.itemId.unique(),columns=['itemId']) # df of all unique items\n",
    "    dict_items = items.to_dict()    \n",
    "    inv_dict_items = {v: k for k, v in dict_items['itemId'].items()}\n",
    "\n",
    "    data_encoded.userId = data_encoded.userId.map(inv_dict_users)\n",
    "    data_encoded.itemId = data_encoded.itemId.map(inv_dict_items)\n",
    "\n",
    "    return data_encoded, dict_users, dict_items\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yt6SYVvAX3Di"
   },
   "source": [
    "Here is the procedure we would like to implement in the function SGD():\n",
    "\n",
    "1.   itinialize P and Q to random values\n",
    "\n",
    "2.   for $n_{epochs}$ passes on the data:\n",
    "\n",
    "    *   for all known ratings $r_{ui}$\n",
    "        *   compute the error between the predicted rating $p_u \\cdot q_i$ and the known ratings $r_{ui}$:\n",
    "        $$ err = r_{ui} - p_u \\cdot q_i $$\n",
    "        *   update $p_u$ and $q_i$ with the following rule:\n",
    "        $$ p_u \\leftarrow p_u + \\alpha \\cdot err \\cdot q_i  $$\n",
    "        $$ q_i \\leftarrow q_i + \\alpha \\cdot err \\cdot p_u$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iA0tyBHJ5xyI"
   },
   "outputs": [],
   "source": [
    "# Adapted from http://nicolas-hug.com/blog/matrix_facto_4\n",
    "def SGD(data,           # dataframe containing 1 user|item|rating per row\n",
    "        n_factors = 10, # number of factors\n",
    "        alpha = .01,    # number of factors\n",
    "        n_epochs = 3,   # number of iteration of the SGD procedure\n",
    "       ):\n",
    "    '''Learn the vectors P and Q (ie all the weights p_u and q_i) with SGD.\n",
    "    '''\n",
    "\n",
    "    # Encoding userId's and itemId's in data\n",
    "    data, dict_users, dict_items = encode_ids(data)\n",
    "    \n",
    "    ##### FILL HERE (2 lines) ######\n",
    "    n_users = NULL  # number of unique users\n",
    "    n_items = NULL  # number of unique items\n",
    "    ################################\n",
    "    \n",
    "    # Randomly initialize the user and item factors.\n",
    "    p = np.random.normal(0, .1, (n_users, n_factors))\n",
    "    q = np.random.normal(0, .1, (n_items, n_factors))\n",
    "\n",
    "    # Optimization procedure\n",
    "    for epoch in range(n_epochs):\n",
    "        print ('epoch: ', epoch)\n",
    "        # Loop over the rows in data\n",
    "        for index in range(data.shape[0]):\n",
    "            row = data.iloc[[index]]\n",
    "            u = int(row.userId)      # current userId = position in the p vector (thanks to the encoding)\n",
    "            i = int(row.itemId)      # current itemId = position in the q vector\n",
    "            r_ui = float(row.rating) # rating associated to the couple (user u , item i)\n",
    "            \n",
    "            ##### FILL HERE (1 line) ######\n",
    "            err = NULL    # difference between the predicted rating (p_u . q_i) and the known ratings r_ui\n",
    "            ################################\n",
    "            \n",
    "            # Update vectors p_u and q_i\n",
    "            ##### FILL HERE (2 lines) ######\n",
    "            p[u] = NULL  # cf. update rule above \n",
    "            q[i] = NULL\n",
    "            ################################\n",
    "            \n",
    "    return p, q\n",
    "    \n",
    "    \n",
    "def estimate(u, i, p, q):\n",
    "    '''Estimate rating of user u for item i.'''\n",
    "    ##### FILL HERE (1 line) ######\n",
    "    return NULL             #scalar product of p[u] and q[i] /!\\ dimensions\n",
    "    ################################  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MYUUm18-id6"
   },
   "outputs": [],
   "source": [
    "p, q = SGD(df_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJd80gNgNuUR"
   },
   "source": [
    "## Get the estimate for all user-item pairs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hj4Pc-FjPJK6"
   },
   "source": [
    "Get the user-item matrix filled with predicted ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRCg3k2IPMSc"
   },
   "outputs": [],
   "source": [
    "df_user_item_filled = pd.DataFrame(np.dot(p, q.transpose()))\n",
    "df_user_item_filled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLHPMdpyN96R"
   },
   "source": [
    "However, it is using the encode ids ; we need to retrieve the association of encoded ids to original ids, and apply it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuft25TRN4CY"
   },
   "outputs": [],
   "source": [
    "df_ratings_encoded, dict_users, dict_items = encode_ids(df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCidjCrUl2tx"
   },
   "outputs": [],
   "source": [
    "df_user_item_filled.rename(columns=(dict_items['itemId']), inplace=True)\n",
    "df_user_item_filled.rename(index=(dict_users['userId']), inplace=True)\n",
    "\n",
    "# Sort index/rows (userId's) and columns (itemId's)\n",
    "df_user_item_filled.sort_index(axis=0, inplace=True)\n",
    "df_user_item_filled.sort_index(axis=1, inplace=True)\n",
    "\n",
    "df_user_item_filled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVXIqXAdOPzX"
   },
   "source": [
    "Originally available ratings for user 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyka6nXcOPo4"
   },
   "outputs": [],
   "source": [
    "df_user_item.loc[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pphixa2wOPeh"
   },
   "source": [
    "Estimated ratings after the approximate SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDczh7x5Q6in"
   },
   "outputs": [],
   "source": [
    "df_user_item_filled.loc[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk8zB0HCmLvk"
   },
   "source": [
    "## Give recommendation to a user\n",
    "\n",
    "For instance 10 recommended movies for user 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8zxuZ2VmaIs"
   },
   "outputs": [],
   "source": [
    "recommendations = list((df_user_item_filled.loc[10]).sort_values(ascending=False)[:10].index)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U7R7lyTuOy_"
   },
   "outputs": [],
   "source": [
    "df_movies[df_movies.itemId.isin(recommendations)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fhXmfLeuDZo"
   },
   "source": [
    "vs the ones that were rated initially:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ooeCcRnuI8y"
   },
   "outputs": [],
   "source": [
    "already_rated = list((df_user_item.loc[10]).sort_values(ascending=False)[:10].index)\n",
    "already_rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SM3mJYwyF1g"
   },
   "outputs": [],
   "source": [
    "df_movies[df_movies.itemId.isin(already_rated)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKarQdgbm4tw"
   },
   "source": [
    "This is all the movies in descending order of predicted rating. Let's remove the ones that where alread rated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkvVcbTALIji"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "To put this into production, you'd first separate data into a training and validation set and optimize the number of latent factors (n_factors) by minimizing the Root Mean Square Error. \n",
    "It is easier to use a framework that allows to do this, do cross-validation, grid search, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMdbrNdLldG9"
   },
   "source": [
    "# Gradient Descent SVD using Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VdMT5PnbIn9"
   },
   "outputs": [],
   "source": [
    "!pip install surprise\n",
    "#!pip install scikit-surprise # if the first line does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ed0lnuff4NOw"
   },
   "outputs": [],
   "source": [
    "# from surprise import Reader, Dataset, SVD, evaluate\n",
    "\n",
    "# Following Surprise documentation examples \n",
    "# https://surprise.readthedocs.io/en/stable/getting_started.html\n",
    "\n",
    "from surprise import Reader, Dataset, SVD, evaluate, NormalPredictor\n",
    "from surprise.model_selection import cross_validate\n",
    "from collections import defaultdict\n",
    "\n",
    "# As we're loading a custom dataset, we need to define a reader.\n",
    "reader = Reader(rating_scale=(0.5, 5))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df_ratings[['userId', 'itemId', 'rating']], reader)\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyciPjWI4Q94"
   },
   "source": [
    "#### Tune algorithm parameters with GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tG3nlrAKzLZg"
   },
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n",
    "              'reg_all': [0.4, 0.6]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnfvwVPvzUsw"
   },
   "outputs": [],
   "source": [
    "# We can now use the algorithm that yields the best rmse:\n",
    "algo = gs.best_estimator['rmse']\n",
    "trainset = data.build_full_trainset()\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVAeYFgTzppL"
   },
   "outputs": [],
   "source": [
    "algo.predict(621,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "li7UhY6fz1oG"
   },
   "outputs": [],
   "source": [
    "df_data = data.df\n",
    "df_data = df_data.join(df_movies,how=\"left\", on='itemId',rsuffix='_', lsuffix='')\n",
    "df_data[df_data['userId']==1].sort_values(by = 'rating',ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRm97oJVz8wG"
   },
   "outputs": [],
   "source": [
    "# From Surprise documentation: https://surprise.readthedocs.io/en/stable/FAQ.html\n",
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poADsLk634aR"
   },
   "outputs": [],
   "source": [
    "# Predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zn3AViRh19eR"
   },
   "outputs": [],
   "source": [
    "top_n = get_top_n(predictions, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igRXlPxr4gCH"
   },
   "outputs": [],
   "source": [
    "top_n.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2ElCZzT4EC1"
   },
   "outputs": [],
   "source": [
    "# Print the recommended items for all user 1\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])\n",
    "    if uid == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OVCCW1C4ziF"
   },
   "outputs": [],
   "source": [
    "df_movies[df_movies.itemId.isin([318, 750, 1204, 858, 904, 48516, 1221, 912, 1276, 4973])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNVZSfS35PSo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RecSys Movie_recommender_system_CF_v2_TOFILL.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/couturierc/tutorials/blob/master/recommender_system/Movie_recommender_system_CF_v2_TOFILL.ipynb",
     "timestamp": 1617530488634
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
