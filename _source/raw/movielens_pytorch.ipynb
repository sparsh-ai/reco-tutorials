{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit feedback implementation of Recommender Systems based on MovieLens 1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notes to properly run the notebook\n",
    "# At the time of developping this notebook, tensorboard was not fully integrated in skorch\n",
    "# so it has to be installed from the sources\n",
    "# git clone https://github.com/skorch-dev/skorch.git && cd skorch && python setup.py install\n",
    "\n",
    "# The ipywidgets package needs to be installed to see the progressbar checkpoint\n",
    "# It also needs to be activated like this: jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important notes for JSGL\n",
    "# If doing gridsearch, don't activate the function from dataloaders that spawn multiprocesses, memory will be hogged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQZgn3JNuFXM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import patsy\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skorch import NeuralNet\n",
    "from skorch.helper import predefined_split, SliceDataset\n",
    "from skorch.callbacks import BatchScoring, Checkpoint, EarlyStopping, EpochScoring, LRScheduler, TensorBoard, ProgressBar\n",
    "\n",
    "# Install latest Tensorflow build\n",
    "#!pip install -q tf-nightly-2.0-preview\n",
    "import tensorflow as tf\n",
    "from tensorflow import summary\n",
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MYaQB02n1Md_",
    "outputId": "e0fedd2a-edd8-4fa4-c812-18752d9e332b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device  cpu\n",
      "Using torch version  1.2.0\n"
     ]
    }
   ],
   "source": [
    "# Torch parameters\n",
    "identifier = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(identifier)\n",
    "device = 'cpu'\n",
    "print('Using device ', device)\n",
    "\n",
    "print('Using torch version ', torch.__version__)\n",
    "\n",
    "torch.set_printoptions(precision=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "ZvYWqFf2uG9j",
    "outputId": "0a6594ad-c7da-407a-ca5a-b34cbbefbc86"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('ml-1m'):\n",
    "    !wget http://files.grouplens.org/datasets/movielens/ml-1m.zi\n",
    "    !unzip -o ml-1m.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j24pjXi6tMs6"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QIFR-8xl-kQ8"
   },
   "outputs": [],
   "source": [
    "class rsdataset(Dataset):\n",
    "    def __init__(self, usersfile, moviesfile, ratingsfile, nrows=None):\n",
    "        \n",
    "        # Read files\n",
    "        self.movies = pd.read_csv(moviesfile, sep='::', names=['MovieID', 'Title', 'Genres'], engine='python')\n",
    "        self.users = pd.read_csv(usersfile, sep='::', names=['UserID', 'Gender', 'Age', 'Occupation', 'Zipcode'], engine='python')\n",
    "        self.ratings = pd.read_csv(ratingsfile, sep='::', names=['UserID', 'MovieID', 'Rating', 'Timestamp'], engine='python', nrows=nrows)\n",
    "        \n",
    "        assert self.users['UserID'].nunique() >= self.ratings['UserID'].nunique(), 'UserID with unknown information'\n",
    "        assert self.movies['MovieID'].nunique() >= self.ratings['MovieID'].nunique(), 'Movies with unknown information'\n",
    "\n",
    "        self.users_emb_columns = []\n",
    "        self.users_ohe_columns = []\n",
    "        self.movies_emb_columns = []\n",
    "        self.movies_ohe_columns = []\n",
    "        self.interact_columns = []\n",
    "\n",
    "        self.nusers = self.ratings['UserID'].nunique()\n",
    "        self.nmovies = self.ratings['MovieID'].nunique()\n",
    "\n",
    "        self.y_range = (self.ratings['Rating'].min(), self.ratings['Rating'].max())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        What have we learned regarding tensors and GPU memory\n",
    "        -----------------------------------------------------\n",
    "        - For every type of data, use the smalleset memory size required. For example, don't use int64 for ohe.\n",
    "        - pinned_memory=True didn't help my speed problems when I tried.\n",
    "        - As a consequence, everything was put in memory, and I __getitem__ was used to slice data.\n",
    "        - num_workers helped improving the speed. \n",
    "        \"\"\"\n",
    "        return (((self.users_emb[idx])),\n",
    "                ((self.users_ohe[idx])),\n",
    "                ((self.movies_emb[idx])),\n",
    "                ((self.movies_ohe[idx])),\n",
    "                ((self.interact[idx]))), (self.y[idx])\n",
    "\n",
    "    def to_tensor(self):\n",
    "        self.users_emb = torch.from_numpy(self.ratings[self.users_emb_columns].values)\n",
    "        self.users_ohe = torch.tensor(self.ratings[self.users_ohe_columns].values, dtype=torch.float)\n",
    "        self.movies_emb = torch.from_numpy(self.ratings[self.movies_emb_columns].values)\n",
    "        self.movies_ohe = torch.tensor(self.ratings[self.movies_ohe_columns].values, dtype=torch.float)\n",
    "        self.interact = torch.from_numpy(self.ratings[self.interact_columns].values)\n",
    "        self.y = torch.tensor(self.y.values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uINDlkiMB_Fo"
   },
   "outputs": [],
   "source": [
    "train = rsdataset('ml-1m/users.dat', 'ml-1m/movies.dat', 'ml-1m/ratings.dat', nrows=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O8ivFYVSM3nr"
   },
   "source": [
    "### Preprocessing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u_w1QQER37El"
   },
   "outputs": [],
   "source": [
    "train.ratings = train.ratings.merge(train.movies, left_on='MovieID', right_on='MovieID')\n",
    "train.movies = train.ratings[train.movies.columns]\n",
    "\n",
    "train.ratings = train.ratings.merge(train.users, left_on='UserID', right_on='UserID')\n",
    "train.users = train.ratings[train.users.columns]\n",
    "\n",
    "train.y = train.ratings['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlQhqnXOYcmp"
   },
   "outputs": [],
   "source": [
    "# Label Encode users\n",
    "columns = ['UserID', 'Gender', 'Age', 'Occupation']\n",
    "train.ratings[columns] = train.ratings[columns].apply(preprocessing.LabelEncoder().fit_transform)\n",
    "train.users_emb_columns = train.users_emb_columns + columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZ3jSa_dRMLA"
   },
   "outputs": [],
   "source": [
    "# Label Encode movies\n",
    "columns = ['MovieID']\n",
    "train.ratings[columns] = train.ratings[columns].apply(preprocessing.LabelEncoder().fit_transform)\n",
    "train.movies_emb_columns = train.movies_emb_columns + columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2XcLbiz9lql"
   },
   "outputs": [],
   "source": [
    "# One Hot Encode users\n",
    "#columns = ['Gender', 'Age', 'Occupation', 'Zipcode']\n",
    "columns = ['Gender', 'Age', 'Occupation']\n",
    "ohe = preprocessing.OneHotEncoder(categories='auto', sparse=False, dtype='uint8')\n",
    "ohe.fit(train.ratings[columns])\n",
    "train.ratings = pd.concat([train.ratings, pd.DataFrame(data=ohe.transform(train.ratings[columns]), columns=ohe.get_feature_names(columns))], axis=1)\n",
    "train.users_ohe_columns = ohe.get_feature_names(columns)\n",
    "\n",
    "assert train.ratings[train.users_ohe_columns].max().max()<=1, 'Error with ohe columns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfVq6sd5NqiO"
   },
   "outputs": [],
   "source": [
    "# One Hot Encode movies (non exclusive)\n",
    "genres = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', \n",
    "          'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "for genre in genres:\n",
    "    genre = genre.replace('-', '')\n",
    "    column = 'Genre_' + str(genre)\n",
    "    train.ratings[column] = train.ratings['Genres'].apply(lambda x: 1 if genre in x else 0)\n",
    "    train.movies_ohe_columns.append(column)\n",
    "    \n",
    "assert train.ratings[train.movies_ohe_columns].max().max()<=1, 'Error with ohe columns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTH7wNbZp_jA"
   },
   "outputs": [],
   "source": [
    "int_genres_gender = \"\"\n",
    "for genre in train.movies_ohe_columns:\n",
    "    int_genres_gender = int_genres_gender + '+' +genre + ':Gender'\n",
    "\n",
    "int_genres_age = \"\"\n",
    "for genre in train.movies_ohe_columns:\n",
    "    int_genres_age = int_genres_age + '+' + genre + ':Age'\n",
    "    \n",
    "interact = patsy.dmatrix(\"0 + Gender:Age + Gender:Occupation + Age:Occupation\"+int_genres_gender+int_genres_age, data=train.ratings.astype('object'), return_type='dataframe').astype('int8')\n",
    "interact = interact.astype('uint8')\n",
    "train.ratings = pd.concat([train.ratings, interact], axis=1)\n",
    "train.interact_columns = interact.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRSGsdEmMlPE"
   },
   "outputs": [],
   "source": [
    "# Drop unused columns\n",
    "train.movies.drop(['Title', 'Genres'], inplace=True, axis=1)\n",
    "train.ratings.drop(['Title', 'Genres', 'Zipcode'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wPTkxLKQMWhI"
   },
   "outputs": [],
   "source": [
    "train.to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hm1qo9It3zsb"
   },
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNXPv9DAmHBB"
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "train_size = int(0.8 * len(train))\n",
    "test_size = len(train) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "dataloaders['valid'] = torch.utils.data.DataLoader(valid_dataset, batch_size=4096, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oa-KNh5qkUh8"
   },
   "source": [
    "### Define Pytorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "daekeS2zju4B",
    "outputId": "066d0111-a05c-4d8d-96df-06294888e231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepnwide(\n",
      "  (emb_UserID): Embedding(6040, 60)\n",
      "  (emb_Gender): Embedding(2, 60)\n",
      "  (emb_Age): Embedding(7, 60)\n",
      "  (emb_Occupation): Embedding(21, 60)\n",
      "  (emb_MovieID): Embedding(3706, 60)\n",
      "  (h1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (h2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (h3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (last_layer): Linear(in_features=420, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class deepnwide(nn.Module):\n",
    "    \"\"\"\n",
    "    Hyperparameters:\n",
    "        - module__size_emb\n",
    "        - module__dropout\n",
    "        - module__linear_size\n",
    "}\n",
    "    Best run: -0.9766627748807272 {'lr': 0.001, 'module__dropout': 0.2, 'module__size_emb': 30}\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, users_emb, movies_emb, users_ohe, movies_ohe, interact, size_emb, y_range, dropout, linear_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.name = 'deepnwide'\n",
    "        self.y_range = y_range\n",
    "\n",
    "        # wide part - We don't need to specify nothing here\n",
    "        \n",
    "        # deep\n",
    "        self.emb_UserID = nn.Embedding(len(torch.unique(users_emb[:, 0])), size_emb)\n",
    "        self.emb_UserID.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_Gender = nn.Embedding(len(torch.unique(users_emb[:, 1])), size_emb)\n",
    "        self.emb_Gender.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_Age = nn.Embedding(len(torch.unique(users_emb[:, 2])), size_emb)\n",
    "        self.emb_Age.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_Occupation = nn.Embedding(len(torch.unique(users_emb[:, 3])), size_emb)\n",
    "        self.emb_Occupation.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_MovieID = nn.Embedding(len(torch.unique(movies_emb[:, 0])), size_emb)\n",
    "        self.emb_MovieID.weight.data.uniform_(-.01, .01)\n",
    "\n",
    "        # hidden layers\n",
    "        self.h1 = nn.Linear(5 * size_emb, linear_size)\n",
    "        self.h2 = nn.Linear(linear_size, linear_size)\n",
    "        self.h3 = nn.Linear(linear_size, linear_size)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.dropout3 = nn.Dropout(p=dropout)\n",
    "\n",
    "        # final dense layer \n",
    "        self.last_layer = nn.Linear((interact.shape[1]) + (movies_ohe.shape[1]) + (linear_size), 1)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Assign data\n",
    "        user_emb = X[0]\n",
    "        user_ohe = X[1]\n",
    "        movie_emb = X[2]\n",
    "        movie_ohe = X[3]\n",
    "        interact = X[4]\n",
    "        \n",
    "        UserID = user_emb[:, 0]\n",
    "        Gender = user_emb[:, 1]\n",
    "        Age = user_emb[:, 2]\n",
    "        Occupation = user_emb[:, 3]\n",
    "        MovieID = movie_emb[:, 0]\n",
    "\n",
    "        UserID = self.emb_UserID(UserID)\n",
    "        Gender = self.emb_Gender(Gender)\n",
    "        Age = self.emb_Age(Age)\n",
    "        Occupation = self.emb_Occupation(Occupation)\n",
    "        MovieID = self.emb_MovieID(MovieID)\n",
    "\n",
    "        emb = torch.cat([UserID,\n",
    "                         Age,\n",
    "                         Gender,\n",
    "                         Occupation,\n",
    "                         MovieID],\n",
    "                         dim=1)\n",
    "        \n",
    "        emb = F.relu(self.dropout1(self.h1(emb)))\n",
    "        emb = F.relu(self.dropout2(self.h2(emb)))\n",
    "        emb = F.relu(self.dropout3(self.h3(emb)))\n",
    "\n",
    "        result = self.last_layer(torch.cat([interact.float(), movie_ohe.float(), emb.float()], dim=1))\n",
    "\n",
    "        return (torch.sigmoid(result) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]).squeeze()\n",
    "\n",
    "\n",
    "model = deepnwide(train.users_emb, train.movies_emb, train.users_ohe, train.movies_ohe, train.interact, 60, train.y_range, 0.5, 100)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "MftgRHZB3rFd",
    "outputId": "a2c9651e-2867-49ab-af8a-6479828a8fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twoembeds(\n",
      "  (emb_UserID): Embedding(6040, 15)\n",
      "  (emb_MovieID): Embedding(3706, 15)\n",
      "  (emb_UserID_b): Embedding(6040, 1)\n",
      "  (emb_MovieID_b): Embedding(3706, 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class twoembeds(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Hyperparameters:\n",
    "        module__size_emb\n",
    "    \"\"\"\n",
    "    def __init__(self, size_emb, y_range):\n",
    "        super().__init__()\n",
    "\n",
    "        # set name of model\n",
    "        self.name = 'twoembeds'\n",
    "        self.y_range = y_range\n",
    "\n",
    "        # User and movie embeddings\n",
    "        self.emb_UserID = nn.Embedding(train.nusers, size_emb)\n",
    "        self.emb_MovieID = nn.Embedding(train.nmovies, size_emb)\n",
    "        self.emb_UserID.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_MovieID.weight.data.uniform_(-.01, .01)\n",
    "        \n",
    "        # User and movie embeddings weights\n",
    "        self.emb_UserID_b = nn.Embedding(train.nusers, 1)\n",
    "        self.emb_MovieID_b = nn.Embedding(train.nmovies, 1)\n",
    "        self.emb_UserID_b.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_MovieID_b.weight.data.uniform_(-.01, .01)\n",
    " \n",
    "\n",
    "    def forward(self, X):\n",
    "        user_emb = X[0]\n",
    "        user_ohe = X[1]\n",
    "        movie_emb = X[2]\n",
    "        movie_ohe = X[3]\n",
    "        interact = X[4]\n",
    "\n",
    "        UserID = user_emb[:, 0]\n",
    "        MovieID = movie_emb[:, 0]\n",
    "\n",
    "        user_emb = self.emb_UserID(UserID)\n",
    "        movie_emb = self.emb_MovieID(MovieID)\n",
    "\n",
    "        mult = (user_emb * movie_emb).sum(1)\n",
    "\n",
    "        # add bias\n",
    "        multb = mult + self.emb_UserID_b(UserID).squeeze() + self.emb_MovieID_b(MovieID).squeeze()\n",
    "\n",
    "        multb = multb.float()\n",
    "\n",
    "        return (torch.sigmoid(multb) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]).squeeze()\n",
    "\n",
    "        return multb\n",
    "\n",
    "\n",
    "model = twoembeds(15, train.y_range)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncf(\n",
      "  (gmf_embuserid): Embedding(6040, 60)\n",
      "  (gmf_embgender): Embedding(2, 60)\n",
      "  (gmf_embage): Embedding(7, 60)\n",
      "  (gmf_embocc): Embedding(21, 60)\n",
      "  (gmf_embmovieid): Embedding(3706, 222)\n",
      "  (mlp_embuserid): Embedding(6040, 60)\n",
      "  (mlp_embgender): Embedding(2, 60)\n",
      "  (mlp_embage): Embedding(7, 60)\n",
      "  (mlp_embocc): Embedding(21, 60)\n",
      "  (mlp_embmovieid): Embedding(3706, 60)\n",
      "  (h1): Linear(in_features=318, out_features=200, bias=True)\n",
      "  (h2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (last_layer): Linear(in_features=340, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ncf(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Collaborative Filtering: https://arxiv.org/pdf/1708.05031.pdf\n",
    "    There is a matrix factorization part in this model and a deep learning one.\n",
    "    \n",
    "    Hyper parameters:\n",
    "    - module__size_emb\n",
    "    - module__dropout\n",
    "    - module__linear_size\n",
    "    \"\"\"\n",
    "    def __init__(self, users_emb, movies_emb, users_ohe, movies_ohe, interact, size_emb, dropout, linear_size, y_range):\n",
    "        super().__init__()\n",
    "\n",
    "        # set name of model\n",
    "        self.name = 'ncf'\n",
    "        self.y_range = y_range\n",
    "        \n",
    "        ### GMF part\n",
    "        # user embeddings\n",
    "        self.gmf_embuserid = nn.Embedding(len(torch.unique(users_emb[:, 0])), size_emb)\n",
    "        self.gmf_embuserid.weight.data.uniform_(-.01, .01)\n",
    "        self.gmf_embgender = nn.Embedding(len(torch.unique(users_emb[:, 1])), size_emb)\n",
    "        self.gmf_embgender.weight.data.uniform_(-.01, .01)\n",
    "        self.gmf_embage = nn.Embedding(len(torch.unique(users_emb[:, 2])), size_emb)\n",
    "        self.gmf_embage.weight.data.uniform_(-.01, .01)\n",
    "        self.gmf_embocc = nn.Embedding(len(torch.unique(users_emb[:, 3])), size_emb)\n",
    "        self.gmf_embocc.weight.data.uniform_(-.01, .01)\n",
    "        # movie embeddings\n",
    "        self.gmf_embmovieid = nn.Embedding(len(torch.unique(movies_emb[:, 0])), size_emb*4-len(train.movies_ohe_columns))\n",
    "        self.gmf_embmovieid.weight.data.uniform_(-.01, .01)\n",
    "        \n",
    "        \n",
    "        ### MLP part\n",
    "        # user embeddings\n",
    "        self.mlp_embuserid = nn.Embedding(len(torch.unique(users_emb[:, 0])), size_emb)\n",
    "        self.mlp_embuserid.weight.data.uniform_(-.01, .01)\n",
    "        self.mlp_embgender = nn.Embedding(len(torch.unique(users_emb[:, 1])), size_emb)\n",
    "        self.mlp_embgender.weight.data.uniform_(-.01, .01)\n",
    "        self.mlp_embage = nn.Embedding(len(torch.unique(users_emb[:, 2])), size_emb)\n",
    "        self.mlp_embage.weight.data.uniform_(-.01, .01)\n",
    "        self.mlp_embocc = nn.Embedding(len(torch.unique(users_emb[:, 3])), size_emb)\n",
    "        self.mlp_embocc.weight.data.uniform_(-.01, .01)\n",
    "        # movie embeddings\n",
    "        self.mlp_embmovieid = nn.Embedding(len(torch.unique(movies_emb[:, 0])), size_emb)\n",
    "        self.mlp_embmovieid.weight.data.uniform_(-.01, .01)\n",
    "        # hidden layers\n",
    "        self.h1 = nn.Linear(5*size_emb+len(train.movies_ohe_columns), linear_size)\n",
    "        self.h2 = nn.Linear(linear_size, int(linear_size/2))\n",
    "        #self.h3 = nn.Linear(linear_size, linear_size)\n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        #self.dropout3 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # final dense layer \n",
    "        self.last_layer = nn.Linear(size_emb*4+int(linear_size/2), 1)\n",
    "                                       \n",
    "    def forward(self, X):\n",
    "        user_emb = X[0]\n",
    "        user_ohe = X[1]\n",
    "        movie_emb = X[2]\n",
    "        movie_ohe = X[3]\n",
    "        interact = X[4]\n",
    "        \n",
    "        UserID = user_emb[:, 0]\n",
    "        Gender = user_emb[:, 1]\n",
    "        Age = user_emb[:, 2]\n",
    "        Occupation = user_emb[:, 3]\n",
    "        MovieID = movie_emb[:, 0]\n",
    "\n",
    "        # GMF part\n",
    "        gmf_embuserid = self.gmf_embuserid(UserID)\n",
    "        gmf_embgender = self.gmf_embgender(Gender)\n",
    "        gmf_embage = self.gmf_embage(Age)\n",
    "        gmf_embocc = self.gmf_embocc(Occupation)\n",
    "        gmf_embmovieid = self.gmf_embmovieid(MovieID)\n",
    "\n",
    "        gmf_user_vector = torch.cat([gmf_embuserid,\n",
    "                                    gmf_embgender,\n",
    "                                    gmf_embage,\n",
    "                                    gmf_embocc],\n",
    "                                    dim=1)\n",
    "\n",
    "        gmf_movie_vector = torch.cat([gmf_embmovieid, movie_ohe], 1)\n",
    "        \n",
    "        gmf_vector = (gmf_user_vector * gmf_movie_vector)\n",
    "\n",
    "        \n",
    "        # MLP part\n",
    "        mlp_embuserid = self.mlp_embuserid(UserID)\n",
    "        mlp_embgender = self.mlp_embgender(Gender)\n",
    "        mlp_embage = self.mlp_embage(Age)\n",
    "        mlp_embocc = self.mlp_embocc(Occupation)\n",
    "        mlp_movieid = self.mlp_embmovieid(MovieID)\n",
    "        \n",
    "        mlp_vector = torch.cat([mlp_embuserid,\n",
    "                                mlp_embgender,\n",
    "                                mlp_embage,\n",
    "                                mlp_embocc,\n",
    "                                mlp_movieid,\n",
    "                                movie_ohe],\n",
    "                                dim=1)\n",
    "        mlp_vector = F.relu(self.dropout1(self.h1(mlp_vector)))\n",
    "        mlp_vector = F.relu(self.dropout2(self.h2(mlp_vector)))\n",
    "        #mlp_vector = F.relu(self.dropout3(self.h3(mlp_vector)))\n",
    "\n",
    "        # Fusion\n",
    "        result = torch.cat([gmf_vector, mlp_vector], dim=1)\n",
    "        result = self.last_layer(result)\n",
    "        \n",
    "        #return (torch.sigmoid(result) * (5-1) + 1).squeeze\n",
    "        return (torch.sigmoid(result) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]).squeeze()\n",
    "\n",
    "\n",
    "model = ncf(train.users_emb, train.movies_emb, train.users_ohe, train.movies_ohe, train.interact, 60, 0.5, 200, train.y_range)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_rBEpoBPd4w5"
   },
   "source": [
    "### Skorch callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F9XsVtTYOqGq"
   },
   "outputs": [],
   "source": [
    "# Earlystopping callback\n",
    "earlystopping = EarlyStopping(monitor='valid_loss', patience=10, threshold=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ockpmqrz9KYv"
   },
   "outputs": [],
   "source": [
    "# RMSE callback\n",
    "def rmseloss(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_scorer = make_scorer(rmseloss)\n",
    "\n",
    "epoch_rmse = EpochScoring(rmse_scorer, name='rmse_score', lower_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p62gAk8FgaZS"
   },
   "outputs": [],
   "source": [
    "# Checkpoint callback\n",
    "checkpoint = Checkpoint(monitor='rmse_score_best', f_params='params.pt', f_optimizer='optimizer.pt', f_history='history.json', f_pickle='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "knAy1XQYjgze"
   },
   "outputs": [],
   "source": [
    "# Learning rate scheduler callback\n",
    "lr_scheduler = LRScheduler(policy=\"StepLR\", step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jy29DspnXU-S"
   },
   "outputs": [],
   "source": [
    "# Progressbar callback\n",
    "progressbar = ProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "u4yfIEH2rPZE",
    "outputId": "af7fad5c-f72c-4a7c-f302-0edf072bf0ea"
   },
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "writer = SummaryWriter()\n",
    "#%tensorboard --logdir 'runs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually specify hyperparamers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#86.57\n",
    "ncfnet = NeuralNet(\n",
    "    ncf,\n",
    "    module__users_emb=train.users_emb,\n",
    "    module__movies_emb=train.movies_emb,\n",
    "    module__users_ohe=train.users_ohe,\n",
    "    module__movies_ohe=train.movies_ohe,\n",
    "    module__interact=train.interact,\n",
    "    module__size_emb=60,\n",
    "    module__dropout=0.5,\n",
    "    module__linear_size=600,\n",
    "    module__y_range=train.y_range,#### Manually specify hyperparamers \n",
    "    max_epochs=30,\n",
    "    lr=0.0001,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    device=device,\n",
    "    iterator_train__batch_size=1024,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_valid__batch_size=4096,\n",
    "    train_split=predefined_split(valid_dataset),\n",
    "    callbacks=[\n",
    "               earlystopping,\n",
    "               epoch_rmse,\n",
    "               checkpoint,\n",
    "               lr_scheduler,\n",
    "               #TensorBoard(writer),\n",
    "               #progressbar\n",
    "               ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    rmse_score    train_loss    valid_loss    cp      dur\n",
      "-------  ------------  ------------  ------------  ----  -------\n",
      "      1        \u001b[36m0.9101\u001b[0m        \u001b[32m0.9748\u001b[0m        \u001b[35m0.8283\u001b[0m     +  79.3879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.9033\u001b[0m        \u001b[32m0.8235\u001b[0m        \u001b[35m0.8160\u001b[0m     +  79.1909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001b[36m0.8969\u001b[0m        \u001b[32m0.8071\u001b[0m        \u001b[35m0.8044\u001b[0m     +  79.2098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.8894\u001b[0m        \u001b[32m0.7880\u001b[0m        \u001b[35m0.7910\u001b[0m     +  79.2458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.8830\u001b[0m        \u001b[32m0.7680\u001b[0m        \u001b[35m0.7797\u001b[0m     +  79.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.8780\u001b[0m        \u001b[32m0.7493\u001b[0m        \u001b[35m0.7708\u001b[0m     +  77.4570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.8743\u001b[0m        \u001b[32m0.7327\u001b[0m        \u001b[35m0.7644\u001b[0m     +  77.5174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8        \u001b[36m0.8709\u001b[0m        \u001b[32m0.7182\u001b[0m        \u001b[35m0.7584\u001b[0m     +  76.8035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9        \u001b[36m0.8702\u001b[0m        \u001b[32m0.6996\u001b[0m        \u001b[35m0.7572\u001b[0m     +  75.5094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10        \u001b[36m0.8698\u001b[0m        \u001b[32m0.6976\u001b[0m        \u001b[35m0.7565\u001b[0m     +  75.6890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11        \u001b[36m0.8695\u001b[0m        \u001b[32m0.6961\u001b[0m        \u001b[35m0.7561\u001b[0m     +  74.1125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     12        \u001b[36m0.8693\u001b[0m        \u001b[32m0.6949\u001b[0m        \u001b[35m0.7556\u001b[0m     +  73.5556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     13        \u001b[36m0.8691\u001b[0m        \u001b[32m0.6931\u001b[0m        \u001b[35m0.7553\u001b[0m     +  74.0562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14        \u001b[36m0.8688\u001b[0m        \u001b[32m0.6920\u001b[0m        \u001b[35m0.7549\u001b[0m     +  72.9371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15        \u001b[36m0.8687\u001b[0m        \u001b[32m0.6908\u001b[0m        \u001b[35m0.7546\u001b[0m     +  72.5099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16        \u001b[36m0.8686\u001b[0m        \u001b[32m0.6889\u001b[0m        \u001b[35m0.7545\u001b[0m     +  72.3930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17        \u001b[36m0.8686\u001b[0m        \u001b[32m0.6884\u001b[0m        \u001b[35m0.7545\u001b[0m     +  73.3795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18        \u001b[36m0.8686\u001b[0m        \u001b[32m0.6883\u001b[0m        \u001b[35m0.7544\u001b[0m     +  72.9965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     19        \u001b[36m0.8686\u001b[0m        \u001b[32m0.6879\u001b[0m        \u001b[35m0.7544\u001b[0m     +  73.8248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20        \u001b[36m0.8685\u001b[0m        0.6880        \u001b[35m0.7544\u001b[0m     +  73.7557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     21        \u001b[36m0.8685\u001b[0m        \u001b[32m0.6878\u001b[0m        \u001b[35m0.7543\u001b[0m     +  73.7562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22        \u001b[36m0.8685\u001b[0m        \u001b[32m0.6877\u001b[0m        \u001b[35m0.7543\u001b[0m     +  72.9678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     23        \u001b[36m0.8685\u001b[0m        \u001b[32m0.6877\u001b[0m        \u001b[35m0.7543\u001b[0m     +  73.1643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ncf. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "ncfnet.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "[Parallel(n_jobs=15)]: Using backend LokyBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done   1 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=15)]: Done   2 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=15)]: Done   3 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=15)]: Done   4 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=15)]: Done   5 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=15)]: Done   6 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=15)]: Done   7 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=15)]: Done   8 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=15)]: Done   9 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=15)]: Done  10 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=15)]: Done  11 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=15)]: Done  12 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=15)]: Done  13 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=15)]: Done  14 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=15)]: Done  15 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=15)]: Done  16 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=15)]: Done  17 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=15)]: Done  18 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=15)]: Done  19 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=15)]: Done  21 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=15)]: Done  22 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=15)]: Done  23 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=15)]: Done  24 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=15)]: Done  25 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=15)]: Done  26 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=15)]: Done  27 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=15)]: Done  28 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=15)]: Done  29 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=15)]: Done  30 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=15)]: Done  31 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=15)]: Done  32 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=15)]: Done  33 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=15)]: Done  34 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=15)]: Done  35 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=15)]: Done  36 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=15)]: Done  37 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=15)]: Done  38 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=15)]: Done  39 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=15)]: Done  40 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=15)]: Done  41 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=15)]: Done  42 tasks      | elapsed: 17.7min\n",
      "[Parallel(n_jobs=15)]: Done  43 tasks      | elapsed: 17.8min\n",
      "[Parallel(n_jobs=15)]: Done  44 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=15)]: Done  45 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=15)]: Done  46 tasks      | elapsed: 18.6min\n",
      "[Parallel(n_jobs=15)]: Done  47 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=15)]: Done  48 tasks      | elapsed: 19.2min\n",
      "[Parallel(n_jobs=15)]: Done  49 tasks      | elapsed: 20.7min\n",
      "[Parallel(n_jobs=15)]: Done  50 tasks      | elapsed: 20.9min\n",
      "[Parallel(n_jobs=15)]: Done  51 tasks      | elapsed: 21.1min\n",
      "[Parallel(n_jobs=15)]: Done  52 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=15)]: Done  53 tasks      | elapsed: 21.5min\n",
      "[Parallel(n_jobs=15)]: Done  54 tasks      | elapsed: 21.8min\n",
      "[Parallel(n_jobs=15)]: Done  55 tasks      | elapsed: 22.1min\n",
      "[Parallel(n_jobs=15)]: Done  56 tasks      | elapsed: 22.1min\n",
      "[Parallel(n_jobs=15)]: Done  57 tasks      | elapsed: 22.3min\n",
      "[Parallel(n_jobs=15)]: Done  58 tasks      | elapsed: 23.1min\n",
      "[Parallel(n_jobs=15)]: Done  59 tasks      | elapsed: 23.2min\n",
      "[Parallel(n_jobs=15)]: Done  60 tasks      | elapsed: 23.5min\n",
      "[Parallel(n_jobs=15)]: Done  61 tasks      | elapsed: 24.7min\n",
      "[Parallel(n_jobs=15)]: Done  62 tasks      | elapsed: 25.3min\n",
      "[Parallel(n_jobs=15)]: Done  63 tasks      | elapsed: 26.2min\n",
      "[Parallel(n_jobs=15)]: Done  64 tasks      | elapsed: 27.0min\n",
      "[Parallel(n_jobs=15)]: Done  65 tasks      | elapsed: 27.1min\n",
      "[Parallel(n_jobs=15)]: Done  66 tasks      | elapsed: 27.5min\n",
      "[Parallel(n_jobs=15)]: Done  67 tasks      | elapsed: 28.0min\n",
      "[Parallel(n_jobs=15)]: Done  68 tasks      | elapsed: 28.1min\n",
      "[Parallel(n_jobs=15)]: Done  69 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=15)]: Done  70 tasks      | elapsed: 28.5min\n",
      "[Parallel(n_jobs=15)]: Done  71 tasks      | elapsed: 28.8min\n",
      "[Parallel(n_jobs=15)]: Done  72 tasks      | elapsed: 28.9min\n",
      "[Parallel(n_jobs=15)]: Done  73 tasks      | elapsed: 29.7min\n",
      "[Parallel(n_jobs=15)]: Done  74 tasks      | elapsed: 30.1min\n",
      "[Parallel(n_jobs=15)]: Done  75 tasks      | elapsed: 30.2min\n",
      "[Parallel(n_jobs=15)]: Done  76 tasks      | elapsed: 30.2min\n",
      "[Parallel(n_jobs=15)]: Done  77 tasks      | elapsed: 30.7min\n",
      "[Parallel(n_jobs=15)]: Done  78 tasks      | elapsed: 30.9min\n",
      "[Parallel(n_jobs=15)]: Done  79 tasks      | elapsed: 32.3min\n",
      "[Parallel(n_jobs=15)]: Done  80 tasks      | elapsed: 32.4min\n",
      "[Parallel(n_jobs=15)]: Done  81 tasks      | elapsed: 32.6min\n",
      "[Parallel(n_jobs=15)]: Done  82 tasks      | elapsed: 32.6min\n",
      "[Parallel(n_jobs=15)]: Done  83 tasks      | elapsed: 33.6min\n",
      "[Parallel(n_jobs=15)]: Done  84 tasks      | elapsed: 33.7min\n",
      "[Parallel(n_jobs=15)]: Done  85 tasks      | elapsed: 33.9min\n",
      "[Parallel(n_jobs=15)]: Done  86 tasks      | elapsed: 33.9min\n",
      "[Parallel(n_jobs=15)]: Done  87 tasks      | elapsed: 34.2min\n",
      "[Parallel(n_jobs=15)]: Done  88 tasks      | elapsed: 35.5min\n",
      "[Parallel(n_jobs=15)]: Done  89 tasks      | elapsed: 35.7min\n",
      "[Parallel(n_jobs=15)]: Done  90 tasks      | elapsed: 35.8min\n",
      "[Parallel(n_jobs=15)]: Done  91 tasks      | elapsed: 35.9min\n",
      "[Parallel(n_jobs=15)]: Done  92 tasks      | elapsed: 36.1min\n",
      "[Parallel(n_jobs=15)]: Done  93 tasks      | elapsed: 36.4min\n",
      "[Parallel(n_jobs=15)]: Done  94 tasks      | elapsed: 37.5min\n",
      "[Parallel(n_jobs=15)]: Done  95 tasks      | elapsed: 37.6min\n",
      "[Parallel(n_jobs=15)]: Done  96 tasks      | elapsed: 38.4min\n",
      "[Parallel(n_jobs=15)]: Done  97 tasks      | elapsed: 38.6min\n",
      "[Parallel(n_jobs=15)]: Done  98 tasks      | elapsed: 39.6min\n",
      "[Parallel(n_jobs=15)]: Done  99 tasks      | elapsed: 40.2min\n",
      "[Parallel(n_jobs=15)]: Done 100 tasks      | elapsed: 40.3min\n",
      "[Parallel(n_jobs=15)]: Done 101 tasks      | elapsed: 40.8min\n",
      "[Parallel(n_jobs=15)]: Done 102 tasks      | elapsed: 41.0min\n",
      "[Parallel(n_jobs=15)]: Done 103 tasks      | elapsed: 41.8min\n",
      "[Parallel(n_jobs=15)]: Done 104 tasks      | elapsed: 42.0min\n",
      "[Parallel(n_jobs=15)]: Done 105 tasks      | elapsed: 42.5min\n",
      "[Parallel(n_jobs=15)]: Done 106 tasks      | elapsed: 42.5min\n",
      "[Parallel(n_jobs=15)]: Done 107 tasks      | elapsed: 42.8min\n",
      "[Parallel(n_jobs=15)]: Done 108 tasks      | elapsed: 42.9min\n",
      "[Parallel(n_jobs=15)]: Done 109 tasks      | elapsed: 43.4min\n",
      "[Parallel(n_jobs=15)]: Done 110 tasks      | elapsed: 44.0min\n",
      "[Parallel(n_jobs=15)]: Done 111 tasks      | elapsed: 44.1min\n",
      "[Parallel(n_jobs=15)]: Done 112 tasks      | elapsed: 44.5min\n",
      "[Parallel(n_jobs=15)]: Done 113 tasks      | elapsed: 44.8min\n",
      "[Parallel(n_jobs=15)]: Done 114 tasks      | elapsed: 45.3min\n",
      "[Parallel(n_jobs=15)]: Done 115 tasks      | elapsed: 45.7min\n",
      "[Parallel(n_jobs=15)]: Done 116 tasks      | elapsed: 46.2min\n",
      "[Parallel(n_jobs=15)]: Done 117 tasks      | elapsed: 46.4min\n",
      "[Parallel(n_jobs=15)]: Done 118 tasks      | elapsed: 46.9min\n",
      "[Parallel(n_jobs=15)]: Done 119 tasks      | elapsed: 47.3min\n",
      "[Parallel(n_jobs=15)]: Done 120 tasks      | elapsed: 47.4min\n",
      "[Parallel(n_jobs=15)]: Done 121 tasks      | elapsed: 47.7min\n",
      "[Parallel(n_jobs=15)]: Done 122 tasks      | elapsed: 47.9min\n",
      "[Parallel(n_jobs=15)]: Done 123 tasks      | elapsed: 48.8min\n",
      "[Parallel(n_jobs=15)]: Done 124 tasks      | elapsed: 48.9min\n",
      "[Parallel(n_jobs=15)]: Done 125 tasks      | elapsed: 49.4min\n",
      "[Parallel(n_jobs=15)]: Done 126 tasks      | elapsed: 49.8min\n",
      "[Parallel(n_jobs=15)]: Done 127 tasks      | elapsed: 50.5min\n",
      "[Parallel(n_jobs=15)]: Done 128 tasks      | elapsed: 50.5min\n",
      "[Parallel(n_jobs=15)]: Done 129 tasks      | elapsed: 50.8min\n",
      "[Parallel(n_jobs=15)]: Done 130 tasks      | elapsed: 50.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done 131 tasks      | elapsed: 51.6min\n",
      "[Parallel(n_jobs=15)]: Done 132 tasks      | elapsed: 51.6min\n",
      "[Parallel(n_jobs=15)]: Done 133 tasks      | elapsed: 52.8min\n",
      "[Parallel(n_jobs=15)]: Done 134 tasks      | elapsed: 53.4min\n",
      "[Parallel(n_jobs=15)]: Done 135 tasks      | elapsed: 53.9min\n",
      "[Parallel(n_jobs=15)]: Done 136 tasks      | elapsed: 54.0min\n",
      "[Parallel(n_jobs=15)]: Done 137 tasks      | elapsed: 55.0min\n",
      "[Parallel(n_jobs=15)]: Done 138 tasks      | elapsed: 55.2min\n",
      "[Parallel(n_jobs=15)]: Done 139 tasks      | elapsed: 55.2min\n",
      "[Parallel(n_jobs=15)]: Done 140 tasks      | elapsed: 55.3min\n",
      "[Parallel(n_jobs=15)]: Done 141 tasks      | elapsed: 55.7min\n",
      "[Parallel(n_jobs=15)]: Done 142 tasks      | elapsed: 56.2min\n",
      "[Parallel(n_jobs=15)]: Done 143 tasks      | elapsed: 56.6min\n",
      "[Parallel(n_jobs=15)]: Done 144 tasks      | elapsed: 56.9min\n",
      "[Parallel(n_jobs=15)]: Done 145 tasks      | elapsed: 57.1min\n",
      "[Parallel(n_jobs=15)]: Done 146 tasks      | elapsed: 57.6min\n",
      "[Parallel(n_jobs=15)]: Done 147 tasks      | elapsed: 57.8min\n",
      "[Parallel(n_jobs=15)]: Done 148 tasks      | elapsed: 58.2min\n",
      "[Parallel(n_jobs=15)]: Done 149 tasks      | elapsed: 58.4min\n",
      "[Parallel(n_jobs=15)]: Done 150 tasks      | elapsed: 59.0min\n",
      "[Parallel(n_jobs=15)]: Done 151 tasks      | elapsed: 59.0min\n",
      "[Parallel(n_jobs=15)]: Done 152 tasks      | elapsed: 59.5min\n",
      "[Parallel(n_jobs=15)]: Done 153 tasks      | elapsed: 59.6min\n",
      "[Parallel(n_jobs=15)]: Done 154 tasks      | elapsed: 59.8min\n",
      "[Parallel(n_jobs=15)]: Done 155 tasks      | elapsed: 59.9min\n",
      "[Parallel(n_jobs=15)]: Done 156 tasks      | elapsed: 59.9min\n",
      "[Parallel(n_jobs=15)]: Done 157 tasks      | elapsed: 60.1min\n",
      "[Parallel(n_jobs=15)]: Done 158 tasks      | elapsed: 60.5min\n",
      "[Parallel(n_jobs=15)]: Done 159 tasks      | elapsed: 60.7min\n",
      "[Parallel(n_jobs=15)]: Done 160 tasks      | elapsed: 61.2min\n",
      "[Parallel(n_jobs=15)]: Done 161 tasks      | elapsed: 61.7min\n",
      "[Parallel(n_jobs=15)]: Done 162 tasks      | elapsed: 61.8min\n",
      "[Parallel(n_jobs=15)]: Done 163 tasks      | elapsed: 62.0min\n",
      "[Parallel(n_jobs=15)]: Done 164 tasks      | elapsed: 62.8min\n",
      "[Parallel(n_jobs=15)]: Done 165 tasks      | elapsed: 63.2min\n",
      "[Parallel(n_jobs=15)]: Done 166 tasks      | elapsed: 63.4min\n",
      "[Parallel(n_jobs=15)]: Done 167 tasks      | elapsed: 63.4min\n",
      "[Parallel(n_jobs=15)]: Done 168 tasks      | elapsed: 64.1min\n",
      "[Parallel(n_jobs=15)]: Done 169 tasks      | elapsed: 64.1min\n",
      "[Parallel(n_jobs=15)]: Done 170 tasks      | elapsed: 64.2min\n",
      "[Parallel(n_jobs=15)]: Done 171 tasks      | elapsed: 64.2min\n",
      "[Parallel(n_jobs=15)]: Done 172 tasks      | elapsed: 64.6min\n",
      "[Parallel(n_jobs=15)]: Done 173 tasks      | elapsed: 65.0min\n",
      "[Parallel(n_jobs=15)]: Done 174 tasks      | elapsed: 66.2min\n",
      "[Parallel(n_jobs=15)]: Done 175 tasks      | elapsed: 66.5min\n",
      "[Parallel(n_jobs=15)]: Done 176 tasks      | elapsed: 66.7min\n",
      "[Parallel(n_jobs=15)]: Done 177 tasks      | elapsed: 67.3min\n",
      "[Parallel(n_jobs=15)]: Done 178 tasks      | elapsed: 67.3min\n",
      "[Parallel(n_jobs=15)]: Done 179 tasks      | elapsed: 67.6min\n",
      "[Parallel(n_jobs=15)]: Done 180 tasks      | elapsed: 67.7min\n",
      "[Parallel(n_jobs=15)]: Done 181 tasks      | elapsed: 68.0min\n",
      "[Parallel(n_jobs=15)]: Done 182 tasks      | elapsed: 68.1min\n",
      "[Parallel(n_jobs=15)]: Done 183 tasks      | elapsed: 68.3min\n",
      "[Parallel(n_jobs=15)]: Done 184 tasks      | elapsed: 68.4min\n",
      "[Parallel(n_jobs=15)]: Done 185 tasks      | elapsed: 68.7min\n",
      "[Parallel(n_jobs=15)]: Done 186 tasks      | elapsed: 68.8min\n",
      "[Parallel(n_jobs=15)]: Done 187 tasks      | elapsed: 69.5min\n",
      "[Parallel(n_jobs=15)]: Done 188 tasks      | elapsed: 69.8min\n",
      "[Parallel(n_jobs=15)]: Done 189 tasks      | elapsed: 70.4min\n",
      "[Parallel(n_jobs=15)]: Done 190 tasks      | elapsed: 70.7min\n",
      "[Parallel(n_jobs=15)]: Done 191 tasks      | elapsed: 71.1min\n",
      "[Parallel(n_jobs=15)]: Done 192 tasks      | elapsed: 72.0min\n",
      "[Parallel(n_jobs=15)]: Done 197 out of 216 | elapsed: 72.8min remaining:  7.0min\n",
      "[Parallel(n_jobs=15)]: Done 202 out of 216 | elapsed: 74.5min remaining:  5.2min\n",
      "[Parallel(n_jobs=15)]: Done 207 out of 216 | elapsed: 76.8min remaining:  3.3min\n",
      "[Parallel(n_jobs=15)]: Done 212 out of 216 | elapsed: 77.7min remaining:  1.5min\n",
      "[Parallel(n_jobs=15)]: Done 216 out of 216 | elapsed: 79.2min finished\n",
      "-1.0079917564594745 {'lr': 0.0001, 'module__dropout': 0.2, 'module__linear_size': 400, 'module__size_emb': 30}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'lr': [0.01, 0.001, 0.0001],\n",
    "    'module__size_emb': [30, 60, 120],\n",
    "    'module__dropout': [0.2, 0.5],\n",
    "    'module__linear_size': [100, 150, 200, 400]\n",
    "}\n",
    "gs = GridSearchCV(ncfnet,\n",
    "                  params,\n",
    "                  verbose=50,\n",
    "                  refit=False,\n",
    "                  pre_dispatch=25,\n",
    "                  n_jobs=15,\n",
    "                  cv=3,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "X_ds = SliceDataset(train, idx=0)\n",
    "y_ds = SliceDataset(train, idx=1)\n",
    "gs.fit(X_ds, y_ds)\n",
    "\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep and Wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually specify hyperparamers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qgVTuwedriTI"
   },
   "outputs": [],
   "source": [
    "deepnwidenet = NeuralNet(\n",
    "    deepnwide,\n",
    "    module__users_emb=train.users_emb,\n",
    "    module__movies_emb=train.movies_emb,\n",
    "    module__users_ohe=train.users_ohe,\n",
    "    module__movies_oh#### Manually specify hyperparamers e=train.movies_ohe,\n",
    "    module__interact=train.interact,\n",
    "    module__size_emb=30,\n",
    "    module__y_range=train.y_range,\n",
    "    module__dropout=0.2,\n",
    "    max_epochs=30,\n",
    "    lr=0.001,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    device=device,\n",
    "    iterator_train__batch_size=1024,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_valid__batch_size=4096,\n",
    "    train_split=predefined_split(valid_dataset),\n",
    "    callbacks=[\n",
    "               earlystopping,\n",
    "               epoch_rmse,\n",
    "               #checkpoint,\n",
    "               lr_scheduler,\n",
    "               #TensorBoard(writer),\n",
    "               #progressbar\n",
    "               ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6qm5_snshUi"
   },
   "outputs": [],
   "source": [
    "#deepnwidenet.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "s4Ecaxc4euD1",
    "outputId": "2cea73fa-450e-48aa-8b0f-77e47127abda"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lr': [0.001, 0.01],\n",
    "    'module__size_emb': [30, 60, 120],\n",
    "    'module__dropout': [0.2, 0.5]\n",
    "}\n",
    "gs = GridSearchCV(deepnwidenet,\n",
    "                  params,\n",
    "                  verbose=50,\n",
    "                  refit=False,\n",
    "                  #pre_dispatch=8,\n",
    "                  n_jobs=8,\n",
    "                  cv=3,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "X_ds = SliceDataset(train, idx=0)\n",
    "y_ds = SliceDataset(train, idx=1)\n",
    "gs.fit(X_ds, y_ds)\n",
    "\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two embeddings - Basic matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually specify hyperparamers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_9-m9v_3q03"
   },
   "outputs": [],
   "source": [
    "twoembedsnet = NeuralNet(\n",
    "    twoembeds,\n",
    "    module__size_emb=128,\n",
    "    module__y_range=train.y_range,\n",
    "    max_epochs=30,\n",
    "    lr=0.001,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    device=device,\n",
    "    iterator_train__batch_size=4096,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_valid__batch_size=4096,\n",
    "    train_split=predefined_split(valid_dataset),\n",
    "    callbacks=[earlystopping,\n",
    "               epoch_rmse,\n",
    "               #checkpoint,\n",
    "               lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9B_Z4LA6CPL"
   },
   "outputs": [],
   "source": [
    "twoembedsnet.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "G2QlpxyRTx7b",
    "outputId": "35483166-0d89-4148-c625-50c8884a9903"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lr': [0.001, 0.01],\n",
    "    'module__size_emb': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "}\n",
    "gs = GridSearchCV(twoembedsnet,\n",
    "                  params,\n",
    "                  verbose=50,\n",
    "                  refit=False,\n",
    "                  #pre_dispatch=8,\n",
    "                  n_jobs=8,\n",
    "                  cv=3,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "X_ds = SliceDataset(train, idx=0)\n",
    "y_ds = SliceDataset(train, idx=1)\n",
    "gs.fit(X_ds, y_ds)\n",
    "\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_WP8cIqZQWN"
   },
   "source": [
    "### Benchmark with scikit-surprise SVD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1pjhfBV6ZRUH"
   },
   "outputs": [],
   "source": [
    "#!pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsJ69IyMZTcZ"
   },
   "outputs": [],
   "source": [
    "from surprise import NormalPredictor\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import cross_validate, train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5., 3.,  ..., 1., 4., 4.])"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[dataloaders['train'].dataset.indices][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UG7J_vXTZjcf"
   },
   "outputs": [],
   "source": [
    "user = train[dataloaders['train'].dataset.indices][0][0][:, 0].data.numpy()\n",
    "movie = train[dataloaders['train'].dataset.indices][0][2][:, 0].data.numpy()\n",
    "y = train[dataloaders['train'].dataset.indices][1].data.numpy()\n",
    "df = pd.DataFrame({'user': user, 'movie': movie, 'y': y})\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[['user', 'movie', 'y']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Khqq2oRQwyZ3"
   },
   "outputs": [],
   "source": [
    "data = Dataset.load_from_df(train.ratings.loc[dataloaders['train'].dataset.indices, ['UserID', 'MovieID', 'Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZULF4SZexU81"
   },
   "outputs": [],
   "source": [
    "a = train.ratings.loc[dataloaders['train'].dataset.indices, ['UserID', 'MovieID', 'Rating']]\n",
    "b = pd.DataFrame({'UserID': user, 'MovieID': movie, 'Rating': y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "osa_K_dkckBG"
   },
   "outputs": [],
   "source": [
    "#data = Dataset.load_builtin('ml-1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePIcLN-Vawop"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8917552064774448"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHY401zJgx8I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "movielens",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
